Inhibitory Networks Generate Nonlinear Effects
Propagation of activity in excitatory networks is simple and predictable. Excitation
just generates further excitation, independent of time, wiring complexity,
strength of excitation, or, in fact, any other factor. Positive forces can move the
system only in a forward direction. An excitatory network always converges toward
the same irreversible end despite the different magnitudes or forms of starting
conditions. Inhibitory networks are fundamentally different. To illustrate this
difference, compare chains of excitatory and inhibitory neurons (figure 3.1). Independent
of the details, the evolution of activity in the purely excitatory network is
monotonic excitation. Excitatory neurons connected in series excite each other at
every step, resulting in a chain reaction of ever-increasing activity without global
stability. In contrast, when an inhibitory interneuron at the beginning of the chain
is activated, it will suppress the activity of its target neuron. As a result, the third
interneuron in the chain will be less suppressed by the second interneuron, so the
activity of the third neuron may increase. Neurophysiologists refer to this process
Figure 3.1. Inhibition introduces “hard-to-predict” nonlinearity in cortical circuits. Excitatory
chains (black) produce only monotonically increasing excitation. In contrast, in inhibitory
(gray) and mixed circuits, the spread of activity can be strongly modified, and the
ultimate outcome depends on the fine details of connections and synaptic strengths (arrow,
excitatory; circle, inhibitory). Vertical arrows indicate the magnitude of activity.
Diversity of Cortical Functions: Inhibition 63
2. Systems with multiple nested structures are called hierarchies. The cortex with its rich variety
of neuron types and multiple organization levels is a complex hierarchical system.
3. Shortly after the discovery of feedforward inhibition in the cortex (Buzsáki and Eidelberg,
1981, 1982; Alger and Nicoll, 1982), the fundamental neurophysiological differences between feedback
and feedforward inhibition were emphasized (Buzsáki, 1984; see also Swadlow, 2002).
4. Pouille and Scanziani (2001).
as disinhibition. The disinhibited third neuron, in turn, will suppress its own
downstream target, and so on. Now consider a ring of excitatory neurons with one
or more inhibitory interneurons embedded in the circuit. Input activation brings
about both spreading excitation and inhibition. The firing patterns of the individual
neurons in the ring are hard to predict because their activity strongly depends
on the exact details of the connections. A minor change in some of the parameters
can result in dramatic changes in the firing properties of all partners involved.
This property is known as nonlinearity.
Networks built from both excitatory and inhibitory elements can self-organize
and generate complex properties.2 However, even in the simplest partnership of a
principal cell and interneuron, the pattern of firing depends on the details of
wiring (figure 3.2).3 In a recurrent inhibitory circuit, increased firing of the principal
cell elevates the interneuron’s discharge frequency, and the interneuron, in
turn, may decrease the principal cell’s output, similar to the action of a thermostat.
Stabilization by negative feedback typically comes in the form of various oscillations
(discussed in Cycle 6). In a feedforward inhibitory configuration,
increased discharge of the interneuron, as the primary event, results in decreased
activity of the principal cell. Such simple pairing of excitation and inhibition can
increase the temporal precision of firing substantially. This is because depolarization
of the principal cell, initiated by the excitatory input, is reduced quickly by
the repolarizing effect of feedforward inhibition, narrowing the temporal window
of discharge probability. Fast coupling of the excitatory and inhibitory influences
can bring about submillisecond precision of spike timing.4
Any departure from the simple feedback or feedforward partnership inevitably
Figure 3.2. Negative (inhibitory) feedback provides stability. Feedforward inhibition
dampens (“filters”) the effect of afferent excitation. Lateral inhibition provides autonomy
(segregation) of neurons by suppressing the similarly activated neighboring neurons
(“winner take all”).
64 RHYTHMS OF THE BRAIN
5. Another important source of nonlinearity is derived from the numerous subcortical modulatory
neurotransmitters (Steriade and Buzsáki, 1990; McCormick et al., 1993). Part of the subcortical effect
is mediated by cortical interneurons (Freund, 2003).
6. An oft-used term for such maintained phase transition is “self-organized criticality” in physics.
Per Bak’s fascinating book on self-organized criticality (Bak, 1996) links self-organization to cascading
failures. For criticism of his treatment of the topic, see Jensen (1998).
increases the complexity of the firing patterns of the participating cells. For example,
when two interneurons are activated simultaneously, their combined effect
on the target principal cell depends primarily on the interaction between the interneurons.
Inhibition, as a “negative force,” introduces nonlinear, hard to predict
effects. An extension of feedback inhibition is lateral inhibition. This occurs when
activation of a principal cell recruits an interneuron, which in turn suppresses the
activity of the surrounding principal cells. Suppose that two principal cells are excited
by the same input, but the input to principal cell A is slightly stronger than
the input to principal cell B. If neuron A and B share a common inhibitory interneuron,
the gain in neuron A results in a suppression of neuron B’s activity.
The same outcome occurs if the input strengths to neurons A and B are equal but
the synapse between neuron A and the interneuron is slightly stronger than that
between neuron B and the interneuron. The initial minor difference in the inputs
results in a very large difference in the output of the two neurons. The same asymmetry
can be produced if input to neuron A arrives slightly earlier than the input
to neuron B. This increased autonomy by competition is also known as “winnertake-all”
mechanism, a nonlinear selection or segregation mechanism.
Speaking more generally, cortical networks gain their nonlinearity and functional
complexity primarily from the inhibitory interneuron system.5 Such complex
interactions between the excitatory and inhibitory neuron pools have at least
two useful consequences. First, principal cells will neither be trapped in repeated
excitatory avalanches nor become completely suppressed, unable of responding
to inputs. Instead, in real networks the set point is somewhere in the middle, so
principal cells embedded in cortical networks are able to react robustly, when
needed, even to the weakest physiological input. In physics, such critical state is
referred to as phase transition, because external forces can shift the system in either
direction. A textbook example of a state transition is the shift between water
and ice. A slight change in temperature (an externally imposed influence) can
shift the state in either direction. If a system, for example, a neural network, can
self-organize in such a way as to maintain itself near the phase transition, it can
stay in this “sensitized” or metastable state until perturbed.6 Despite being maximally
sensitized to external perturbations, neuronal networks with multiple levels
of excitatory and inhibitory constituents are resilient systems, capable of absorbing
large external effects without undergoing functional breakdown.
Another fundamental service of the inhibitory system is that it provides a
high degree of autonomy for individual principal cells or cell groups. Cooperation
of interneurons in the same “class” (see discussion on diverse classes below)
can secure the spatiotemporal segregation of principal cells to perform a
Diversity of Cortical Functions: Inhibition 65
given function. As discussed repeatedly in subsequent Cycles, the most basic
functions accomplished by neuronal networks are pattern completion and pattern
separation, functions related to the concepts of integration and differentiation.
Separation of inputs in a network with only excitatory connections is not
possible. However, with inhibitory connections, the competing cell assemblies
and even neighboring excitatory neurons can be functionally isolated, and excitatory
paths can be rerouted by the traffic-controlling ability of coordinated interneuron
groups. The specific firing patterns of principal cells in a network
thus depend on the temporal and spatial distribution of inhibition. As a result, in
response to the same input, the same network can produce different output patterns
at different times, depending on the state of inhibition (figure 3.3). The coordinated
inhibition ensures that excitatory activity recruits the right numbers
of neurons in the right temporal window and that excitation spreads in the right
direction. None of these important features can be achieved by principal cells
alone.
Interneurons Multiply the Computational Ability
of Principal Cells
The term “cortical interneuron” dates back to times when inhibitory neurons were
thought to provide only somatic feedback inhibition onto local pyramidal cells.
Because their short-range connections were alleged to be the rule, an alternative
term “local circuit interneuron” was also in use for a while. Some interneurons,
however, do project as far as principal cells. Nevertheless, the term “cortical interneuron”
has been preserved with extended roles, much like the now divisible
atom in physics (Greek a-tom, cannot be cut further). Because all known cortical
Figure 3.3. Inhibition is essential for cell assembly selection. Slight differences in
synaptic strengths between the afferent input and neurons in assembly 1 and 2 can completely
silence the competing assembly. In case of equal input strengths, the earlier input
selects the assembly and silences the competing assembly by feedforward and lateral
inhibition.
66 RHYTHMS OF THE BRAIN
7. See Csicsvari et al. (1998). Gulyás et al. (1993b) provides electron microscopic evidence that
excitatory postsynaptic potentials (EPSPs) in interneurons can be reliably evoked by single presynaptic
spikes through just a single release site; see Barthó et al. (2004) and Silberberg et al. (2004) for related
observations in the neocortex.
8. In addition to spike-related release of GABA, the inhibitory neurotransmitter is also released
“spontaneously” in the absence of presynaptic action potential. The exact functional role of these tiny
inhibitory currents (dubbed as “minis”) is not well understood, but they may contribute to the stability
of cortical networks (Nusser and Mody, 2002; Mody and Pearce, 2004).
interneurons, which make up less than one-fifth of the cortical neuronal population,
release the inhibitory neurotransmitter gamma-aminobutyric acid (GABA),
the term “inhibitory interneuron” unambiguously defines the inhibitory cell population
in the cerebral cortex.
How can such a minority group keep in check the excitatory effects brought
about by the majority principal cells in cortical networks? Interneurons deploy
numerous mechanisms to meet this challenge. In contrast to the typically weak
synaptic connections between principal cells, principal cell–interneuron connections
are strong. In the return direction, a typical interneuron innervates a principal
cell with 5–15 synaptic terminals (or boutons). Furthermore, almost half of
the inhibitory terminals are placed at strategically critical positions for controlling
action potential output. On the axon initial segment and cell body of principal
cells, there are only inhibitory synapses supplied by several chandelier and basket
interneurons. The threshold for action potential generation is much lower in interneurons,
and often a single action potential of a presynaptic principal cell is
sufficient to discharge an interneuron, as Jozsef Csicsvari, a graduate student in
my laboratory, has shown.7 As a result, basket and chandelier interneurons work
harder, and their overall firing rate is several times higher than that of the principal
cells, such that the total number of inhibitory postsynaptic potentials (IPSPs)
per unit time, impinging upon a typical principal cell, approximately matches the
effects of the excitatory postsynaptic potentials (EPSPs).8
However, both the kinetics and the spatial distribution of IPSPs and EPSPs are
remarkably different. The rise time and decay time of IPSPs are much faster, and
their amplitude is larger than those of EPSPs. This faster kinetics is the main reason
why interneurons are so much more efficient in timing the action potentials of
pyramidal neurons than are excitatory inputs from other pyramidal cells. Excitatory
potentials dominate the dendrites of principal cells, whereas only IPSPs impinge
upon the cell body (soma) (figure 3.4). The result of this arrangement is
reflected by the larger power of high-frequency currents in the extracellular space
in the somatic layers (where the cell bodies concentrate), relative to the dendritic
layers (to where most excitatory inputs arrive).
It is through the opposing forces of excitation by principal cells and inhibition
by interneurons that the tensegrity harmony of cortical activity is established.
This balanced partnership ensures an overall homeostatic regulation of global firing
rates of neurons over extended territories of the cortex and at the same time
allows for dramatic increases of local excitability in short time windows, necessary
for sending messages and modifying network connections. Balance and feedback
Diversity of Cortical Functions: Inhibition 67
Figure 3.4. The perisomatic (output) region of pyramidal cells is fully controlled by
GABAergic inhibition. The proportion of GABAergic synapses decreases, whereas the
number of spines (associated mainly with excitatory synapses) increases as a function of
distance from the soma. Modified, with permission, from Papp et al. (2001).
9. Mainen and Sejnowski (1996). Morphological features, and likely the associated differences of
channel distributions, may be responsible for the differences between neuronal subtypes within the
same layers, e.g., layer 5 bursting and nonbursting pyramidal neurons (Connors and Regehr, 1996).
control are also essential principles for oscillations, and interneuron networks are
the backbone of many brain oscillators.
In Cycle 2, I mentioned that the five main principal-cell types have distinct
functional properties. This distinctness results from the unique combination of
ion channels in the membrane and from their morphological individuality.
Zachary Mainen and Terry Sejnowski at the Salk Institute have shown that the
biophysical behavior of their computer-model neurons could be changed dramatically
by altering their morphology.9 For example, a neuron with a large or small
dendritic arbor and neurons with similar geometry but different distribution of ion
channels will generate a different output in response to the same input. The extensive
computational capacity of a single principal cell is seldom utilized at once.
Dividing its full computational power into numerous subroutines that could be
flexibly used according to momentary needs would be an enormous advantage.
This important service is provided with ease by the interneuron system. Interneurons
can functionally “eliminate” a dendritic segment or a whole dendrite, selectively
inactivate Ca2+ channels, and segregate dendrites from the soma or the
soma from the axon. In effect, such actions of interneurons are functionally
equivalent to replacing a principal cell with a morphologically different type,
thus functionally increasing component diversity of the principal-cell population
68 RHYTHMS OF THE BRAIN
Figure 3.5. Inhibition can alter the firing patterns of neurons. Top: Burst firing pattern of
a layer 5 model neuron. Bottom: Removal of the apical dendritic tree in the model neuron
converts the burst discharge into a regular firing pattern. A similar effect in firing pattern
can occur when the apical dendritic tree is isolated from the rest of the neuron by proximal
dendritic inhibition. Model firing patterns are modified, with permission, from Mainen and
Sejnowski (1996).
10. Alvarez de Lorenzana and Ward (1987) distinguish between combinatorial expansion (linear)
and generative condensation (nonlinear) for describing general properties of evolving systems.
(figure 3.5). And the large family of interneuron species perform all these tricks in
a matter of milliseconds.
Diversity of Cortical Interneurons
Brain systems with “simple” computational demands evolved only a few neuron
types. For example, the thalamus, basal ganglia, and the cerebellum possess a low
degree of variability in their neuron types. In contrast, cortical structures have
evolved not only five principal-cell types but also numerous classes of GABAergic
inhibitory interneurons. Every surface domain of cortical principal cells is under
the specific control of a unique interneuron class. This is a clever way of
enormously multiplying the functional repertoire of principal cells using mostly
local interneuron wiring. Adding more interneurons of the same type linearly increases
the network’s combinatorial properties. However, adding novel interneuron
types to the old network, even in small numbers, offers a nonlinear expansion
of qualitatively different possibilities.10
Diversity of Cortical Functions: Inhibition 69
11. Interneuron classification advanced first in the hippocampus. See Halasy and Somogyi (1993),
Buhl et al. (1994), Gulyás et al. (1993a), Sík et al. (1994, 1995), and Freund and Buzsáki (1996). For
more recent progress, see the Interneuron Diversity series in Trends in Neuroscience (Mott and Dingledine,
2003; Freund, 2003; Maccaferri and Lacaille, 2003; Lawrence and McBain, 2003; Whittington
and Traub, 2003; Jonas et al., 2004; Baraban and Tallent, 2004; Buzsáki et al., 2004; Monyer and
Markram, 2004; Cossart et al., 2005). The most comprehensive treatment of interneuron diversity is
the excellent book Diversity in the Neuronal Machine (Soltesz, 2006).
12. To date, the various classes, their connectivity, and functions are best characterized in the hippocampus,
a cortical structure with a single principal-cell layer, because the full extent of the dendritic
and axon arbors of in vivo labeled hippocampal interneurons has been quantified, and their physiological
features have been extensively characterized in both slice preparations and behaving animals (Freund
and Buzsáki, 1996; Klausberger et al., 2003, 2004). The wiring and functional principles derived
from hippocampal interneurons appear to be identical or very similar in the isocortex (Somogyi et al.,
1998; Markram et al., 2004; Somogyi and Klausberger, 2005).
13. This classification is based on the concept that the “goal” of inhibition is to provide the
required spatiotemporal autonomy (segregation) for groups of pyramidal cells to execute a given
function.
14. This beautiful name was coined by János Szentágothai (1975; Szentágothai and Arbib, 1974).
He believed that the chandelier-like distribution of this neuron’s boutons corresponded to dendritic
synapses. It was Somogyi who, using his innovative combination of Golgi sections and electron microscopy,
recognized that all boutons terminate on the axon initial segment of pyramidal cells (Somogyi
et al., 1983). He introduced a new term, axoaxonic cell, but the more poetic chandelier cell is still
widely used. A recent, unexpected finding is that chandelier cells may, in fact, depolarize the axon initial
segment and thereby synchronize their target cells (Szabadics et al., 2006).
Our view on cortical interneurons has changed dramatically during the past
decade. What used to be thought of as a homogeneous collection of neurons providing
negative feedback to the principal cells turned out to represent a large family
of intrinsically different cells with unexpectedly complex circuit wiring. To
date, there is not even a widely accepted taxonomy of interneurons, and novel
types are being discovered literally monthly. Splitters and lumpers like to divide
interneurons into, respectively, infinite or small numbers of categories. Péter Somogyi
at Oxford University, Tamás Freund at the Hungarian Academy of Sciences,
Budapest, and I suggested that the axonal targets of interneurons on the
principal cells should be the first main division of interneuron classification.11
The functional justification of this classification is that the main goal of the interneuron
system is to enhance and optimize the computational abilities of the
principal cells.12 In their relation to the principal cells, three major interneuron
families are recognized (figure 3.6).13 The first and largest family of interneurons
controls the output of principal cells by providing perisomatic inhibition. Output
control is achieved at either the soma by basket cells or the axon initial segment
by chandelier cells.14 Interneurons of the second family target specific dendritic
domains of principal cells. Every known excitatory pathway in the cortex has a
matching family of interneurons. Several additional subclasses seek out two or
more overlapping or nonoverlapping dendritic regions, and yet other subclasses
innervate the somata and nearby dendrites with similar probability. Because the
different domains of principal cells have different functional dynamics, interneurons
innervating those specific domains adapted their kinetic properties to match
70 RHYTHMS OF THE BRAIN
Figure 3.6. The basic cortical circuit, including one type of pyramidal cell (P) and representative
interneuron classes. Perisomatic control of pyramidal cell is secured by basket
and axoaxonic (chandelier) neurons. Both pyramidal cells and interneurons are innervated
by extracircuit excitatory and inhibitory inputs as well as by subcortical neurotransmitters:
acetylcholine (ACh), dopamine (DA), norepinephrine (NA), and serotonin (5-HT, 5-
hydroxytryptamine). Modified, with permission, from Somogyi et al. (1998).
15. Before the landmark paper of Watts and Strogatz (1998) appeared, we reported on the “most peculiar
anatomical features,” as a reviewer of the manuscript put it, of a newly discovered interneuron
type in the hippocampus (Sík et al., 1994). The flow of information in the hippocampus is mostly unidirectional,
or so-called feedforward. The axons of our new neuron, in contrast, went in all directions,
contacting neurons in all subregions of the hippocampus. We suggested that a few long-range neurons
are sufficient to synchronize large territories of local networks. Long-range neurons have also been described
in layers 2 and 6 of the neocortex. Their extensive axon trees cross cortical regions and connect
similar regions of the two hemispheres (Peters et al., 1990; McDonald and Burkhalter, 1993). Similar to
hippocampal long-range interneurons, most of them contain somatostatin immunoreactivity, neuropeptide
Y, and/or neuronal nitric oxide synthase (Tomioka et al., 2005). Axon collaterals of some of the
Martinotti cells (Martinotti, 1889) in the neocortex have been observed to enter the white matter.
their targets. Not surprisingly, members of the dendrite-targeting interneuron
family display the largest variability.
In addition to affecting the activity of principal cells, interneurons also innervate
each other by an elaborate scheme and affect each other’s biophysical
properties. An important subgroup with at least some overlap with the
dendrite-targeting family represents a special set of interneurons whose axon
trees span two or more anatomical regions, and some axon collaterals cross the
hemispheric midline and/or innervate subcortical structures, hence the term
“long-range” interneuron.15 Their distant clouds of terminal boutons are separated
by myelinated axon collaterals that provide fast conduction speed for
temporal synchrony of all terminals (figure 3.7). Such widely projecting, longrange
neurons are rare, but in light of the functional importance of small-world
Diversity of Cortical Functions: Inhibition 71
Figure 3.7. Axon collaterals of GABAergic interneurons can span different anatomical
regions. The interneuron shown here projects back from the hippocampal cornu ammonis
1 (CA1) region to the dentate gyrus (DG) and the CA3 regions. Similar long-range interneurons
project to subcortical sites, the contralateral hippocampus, or the entorhinal
cortex. Reprinted, with permission, from Sík et al. (1994).
16. Gulyás et al. (1996) and Freund and Gulyás (1997).
17. This taxonomy is based mostly on interneuron classes of the hippocampus, but it also holds in
the neocortex (Somogyi et al., 1998; Markram et al., 2004).
graphs, their role must be absolutely critical. They provide the necessary conduit
for synchronizing distantly operating oscillators and allow for coherent
timing of a large number of neurons that are not connected directly with each
other.
The third distinct family of interneurons, discovered by Freund’s group, has
the distinguishing characteristics that their axons avoid principal cells and contact
exclusively other interneurons.16 The existence of these interneuron-specific interneurons
provides mounting support for a unique organization of the inhibitory
system. No principal cells are known that contact only other principal cells and
avoid inhibitory interneurons. The interneuron-specific family also overlaps with
the long-range subclass, again emphasizing the importance of interregional synchronization
of inhibition, and consequent coherent oscillatory entrainment of
their target principal-cell populations.17
The cell bodies and dendrites of interneuron families in the first divisions of
our taxonomy can be found in different layers, and their differential inputs can
72 RHYTHMS OF THE BRAIN
18. For a recent review, see Somogyi and Klausberger (2005).
19. Gap junctions tend to occur within the same types of interneurons (Katsumaru et al., 1988;
Connors and Long, 2004; Hestrin and Galarreta, 2005).
20. Given the high diversity of interneuron types, it is unlikely that all types innervate each and
every pyramidal cell in the cortex (Markram et al., 2004). Thus, in addition to diversify the functions
of single cells, interneurons can diversify microcircuits, as well, by introducing inhomogeneities at
dynamically changing time scales.
21. Thomson (2000a,b), Gupta et al. (2000), and Pouille and Scanziani (2004).
compose the basis of the second division. With perhaps 20 or more distinguished
interneuron types in the rodent cortex, the complexity of their wiring must be
enormous, although the critical details are not yet known.18 Furthermore, interneurons
within the same family can communicate with each other via electrical
synapses. These are pores between adjacent membranes of two neurons, called
gap junctions, that allow bidirectional flow of ions and small molecules.19 In addition
to releasing GABA, interneurons also manufacture various calciumbinding
proteins, such as parvalbumin, calbindin, and calretinin, as well as a
variety of different peptides. Many of these peptides, such as cholecystokinin, somatostatin,
and vasointestinal peptide, are hormones and polypeptides with
known endocrine and blood-flow–regulating roles in the body. They thus not only
are convenient markers for anatomists but also could play hitherto poorly understood
roles in communicating the state of interneurons to the principal cells, glial
cells, and brain vessels.20
The advantage of varying the surface domain innervation of the principal cells
by the different interneuron classes becomes especially clear when temporal dynamics
are also included. The biophysical properties of interneurons vary substantially
across the groups, and as a result, they can be recruited differentially at
different firing frequencies of the principal cells. For example, basket cells respond
with decreasing efficacy when stimulated by high-frequency inputs because of
their “depressing” input synapses, which function as a low-pass frequency filter. In
contrast, several types of dendrite-targeting interneurons fail to generate spike output
when driven at low frequency and require several pulses before they begin to
discharge because their input synapses are of the facilitatory type. These interneurons
therefore can be conceived as a high-pass frequency filter. The consequence
of such dynamics is easy to visualize.21 When a pyramidal neuron discharges at a
low rate, it activates almost exclusively its perisomatic interneurons. On the other
hand, at a higher discharge rate, the somatic inhibition decreases, and inhibition is
shifted to the dendritic domain (figure 3.8). Time is thus transformed into subcellular
space, due to the frequency-filtering behavior of synapses.
The Interneuron System as a Distributed Clock
Despite its multifarious wiring, the principal-cell system alone cannot carry out
any useful computation. It is the inhibitory neuronal network, when coupled to
Diversity of Cortical Functions: Inhibition 73
Figure 3.8. Input frequency determines spatial dominance of inhibition. Left: At a slow
input frequency, feedforward dendritic inhibition is weak. Action potentials in the pyramidal
cell body back-propagate into the dendrite. Right: At fast input frequency, the dendritetargeting
neuron (i1) is potentiated, whereas the drive of the soma-targeting interneuron
(i2) is depressed. The result is decreased inhibition of the soma and increased inhibition of
the dendrite. Back-propagation of the action potential to the pyramidal cell dendrite is attenuated
by the enhanced dendritic inhibition. Pouille and Scanziani (2004) have demonstrated
that fast input activation shifts inhibition from the soma to the dendrites. Dendritic
inhibition, in turn, suppressed somadendritic propagation of the action potential and dendritic
Ca2+ influx (Tsubokawa and Ross, 1996; Buzsáki et al., 1996).
the principal cells, that provides the flexibility needed for the complex operations
of the brain. An important goal of single neurons and neuronal networks is to respond
efficiently but selectively to incoming inputs. In a single cell, the former
goal can be achieved by keeping the so-called “resting membrane potential” of
principal cells just below spike threshold. This task is difficult to achieve due to
the nature of thresholds. The threshold concept is identical to that of the phase
transition between ice and water. In both cases, a minimal external force is needed
to bring about a state change. A difficult problem, implicit in the concept of
threshold, is the neuron’s sensitivity to noise. If the membrane potential was just
below threshold all the time, any minor increase in excitation would discharge the
cell. Furthermore, this would be energetically a very expensive mechanism because
complicated machinery would be required to “clamp” the membrane to a
narrow voltage range against a background of fluctuating temperature, pH, and
other factors in the brain environment. If the membrane is protected from noise
by a more negative resting membrane potential, the production of an action potential
output would require stronger depolarization, which is also energetically
costly. An alternative solution is to move membrane potential up and down in a
coordinated manner across neurons. The only disadvantage of this solution is that
the same external input applied repeatedly will have different consequences in
74 RHYTHMS OF THE BRAIN
22. Such temporal sampling solutions are also used at the behavioral level. To get odor samples in
proper doses, vertebrates rhythmically sniff and arthropods flick their olfactory appendages with characteristic
frequency and duration after detecting an odor. Such active fluctuation of the input greatly
enhances odor detection (Laurent, 1999).
23. The external force, of course, is vital. Networks consisting of inhibitory neurons only cannot
sustain any activity. Sustained activity requires regenerative positive feedback, typically supplied by
recurrent excitation. Networks without recurrent excitatory loops (e.g., the cerebellum) do not possess
spontaneous or self-organized network activity (see Cycle 13).
each case, depending on the centrally coordinated mechanism of threshold adjustment.
There will be short windows of opportunity when the membrane potential
is elevated to just below threshold, alternating with times when the input remains
subthreshold because of the transient hyperpolarized state of the neurons. This
inconvenience, however, is amply balanced by the lower energy cost. Fluctuating
the membrane potential is energetically much less costly than keeping it at a constant
depolarized level.22 The important job of swinging the membrane potential
of principal cells is subcontracted to the interneuron system, and the mechanism
is oscillation.
Balance of opposing forces, such as excitation and inhibition, often gives
rise to rhythmic behavior. Oscillators consisting of only excitatory pyramidal
cells also exist, as is the case when GABAergic receptors are blocked pharmacologically.
In such cases, the frequency of hypersynchonous, epileptic oscillations
is determined primarily by the intrinsic biophysical properties of the
participating pyramidal cells and the time course of neurotransmitter replenishment
after depletion. Under physiological conditions, oscillations critically depend
on inhibitory interneurons. In fact, providing rhythm-based timing to the
principal cells at multiple time scales is one of the most important roles of interneurons.
Let us first consider the simplest possible oscillating network that consists
of similar types of interneurons, for example, synaptically connected basket
cells. Interneuronal networks without an external excitation would not do much,
of course, except remain silent. A transient excitation would generate only a
transient oscillatory response, which would die away quickly. In order to
maintain an oscillation, some external force is needed to generate spiking activity.
Since the only requirement of such an external force is to maintain some
firing, this role can be played by a subcortical neurotransmitter or ambient glutamate
excitation, each of which can maintain a sufficient level of tonic depolarization.
Activity of interneurons, in turn, can give rise to some order. The
simplest case is when all or some interneurons themselves display an oscillatory
response, and inhibitory coupling can link them into an oscillating network.23
However, even if none of the interneurons oscillates in isolation, the synaptically
connected homogeneous interneuron network can still give rise to sustained
oscillations. The intuitive interpretation of collective rhythm in interneuron
Diversity of Cortical Functions: Inhibition 75
24. Because GABAA-receptor–mediated inhibition is mediated by Cl–
, whose equilibrium potential
is close the resting membrane potential, inhibition is not necessarily hyperpolarizing but “shunting”
(i.e., increased membrane conductance). For a contribution of shunting inhibition in oscillations,
see Vida et al. (2006).
25. Inhibition-based oscillators have been known for a long time in simple networks, consisting of
a few neurons only. In such circuits, neurons reciprocally suppress each other’s activity and therefore
spike out of phase (Marder and Calabrese, 1996). In-phase synchrony, brought about by inhibition, has
been demonstrated both in brain slices maintained in vitro and in computer models. For computational
models leading to the above ideas, see Wang and Rinzel (1993), Lytton and Sejnowski (1991), Ermentrout
and Kopell (1998), White et al. (1998a and b), Whittington et al. (1995), Traub et al. (1996,
1999), and Wang and Buzsáki (1996). Inhibitory neurons, in turn, can effectively synchronize target
principal cells (Lytton and Sejnowski, 1991; Buzsáki and Chrobak, 1995; Cobb et al., 1995).
networks is the following. In the initial state, interneurons discharge randomly.
Due to chance, some of them may discharge together in a short time window.
This group of neurons will impose stronger inhibition on their targets than other
randomly discharging neurons. As a result of this stronger inhibitory seed, more
neurons will be silenced simultaneously, after which their probability of discharging
together upon recovery increases.24 Now, we have a larger group of
synchronously discharging cells which, in turn, will silence an even larger portion
of the population, increasing their probability to fire together once inhibition
fades away. With appropriate connectivity and conduction delays, eventually
most or all neurons in the inhibitory network will be inhibited at the same time
and fire synchronously after inhibition wears off. Discharge and silence will alternate
in all parts of the network synchronously. Not all interneurons need to
discharge at every cycle, and the oscillation can be maintained as long as a suffi-
cient portion of interneurons fire at each cycle. The mean time difference between
the discharges of any two pairs of cells is zero; that is, interneurons
discharge synchronously at approximately the same time, independent whether
the cell pairs are connected bidirectionally, one way only, or not at all, as long as
they are part of the same network. The frequency of the oscillation depends only
on the average duration of inhibition, which is the critical time constant in the distributed
interneuron clockwork. If inhibition is mediated by fast-acting GABAA
receptors, the oscillation frequency will correspond to the gamma frequency band
(40–100 hertz). Changing the time constant of the GABAA-receptor–mediated
GABA response will affect the beat frequency of the interneuron network
oscillator.25
Because interneurons connected by GABAA receptors are ubiquitous throughout
the brain, it is not surprising that gamma-frequency oscillation can arise in almost
every structure. In such “gamma clocks,” no single neuron is responsible for
initiating or maintaining the oscillation, yet all of them contribute to the rhythm
whenever they fire. The responsibilities are distributed, and the result depends on
cooperation. Once a collective pattern arises, it constrains the timing of the action
potentials of the individual cells because of the collectively generated inhibition
(figure 3.9). Thus, there are multiple causes/requirements at various levels. Firing
76 RHYTHMS OF THE BRAIN
and connectivity are essential, but the exact wiring is not critical as long as enough
convergence and divergence are present. On the other hand, the oscillation as a
group-level behavior decreases the timing freedom of all neurons. Once the
network is engaged in an oscillation, the convergent inhibition from multiple partners
confines the windows of opportunity for the neurons to discharge. This topdown
constraint is as important as the bottom-up contribution of the individual
members. Therefore, oscillation in the GABAergic interneuron network is a truly
emergent event, governed by both elementary (i.e., bottom-up) and statistical
(top-down) causes.
Let us now add pyramidal neurons to the interneuron network. Intuitively,
what we expect to see is the following. Because of the synchronous discharge of
interneurons, now both interneurons and pyramidal cells are inhibited rhythmically
and at the same time. So if the pyramidal cells are also activated by some
random external force, they discharge with the lowest probability when all neurons
are inhibited and with a higher probability at times when least inhibited,
that is, at the same time as the interneurons. Thus, on average, all neurons will
fire at a zero time lag and will be silenced at the same time. This scenario is best
observed during epileptic discharges and autonomous conditions when outside
influences exert very little effects on the internal pacing of the population. However,
under physiological conditions, oscillator networks made from homogeneous
inhibitory neurons are easy to disrupt because small perturbations in
timing can have a large deteriorating effect on subsequent synchronous discharge
of the neurons. This may explain why gamma-frequency oscillations are
typically short-lasting, transient events. Introducing some heterogeneities, for
Figure 3.9. In networks with only local inhibitory connections, no oscillations emerge
(left: top, spike raster of individual neurons; middle, voltage trace of a single representative
cell; bottom, population synchrony). Adding a small subset of long-range interneurons
to the locally connected population, with 20 percent of the contacts distributed according
to a power-law distribution, robust oscillation emerges (right). Reprinted, with permission,
from Buzsáki et al. (2004).
Diversity of Cortical Functions: Inhibition 77
26. I discuss oscillators based on pyramidal cell–interneuron interactions in Cycle 9.
example, strong pyramidal cell–interneuron coupling, can interfere with the
rules because now locally active pyramidal cells can also affect timing of the interneurons.26
How can a “distributed clock” of neurons with finite axon conduction and
synaptic delays grow in larger brains? Simultaneous inhibition of all neurons is
possible only if inhibition arrives to all neurons more or less at the same time. Inserting
just 1 millisecond of delay between each pair of neurons in a chain or a
two-dimensional lattice of neurons may prevent the coherence of the activity at
high frequencies. Some mechanisms are needed to compensate for the evergrowing
delays. The various solutions that are used to compensate for the delays
in different parts of the brain are discussed in subsequent Cycles. For now, let us
consider how the wiring relationship among the various classes of interneurons
can be maintained in growing brains.
Scaling Interneuron Connections in Growing Brains
The primary role of the interneuron networks is to coordinate timing of the action
potentials. This task becomes more and more complex as the brain grows because
neurons are placed farther apart from each other. Owing to the limited axon conduction
velocities, the growth in volume should somehow be compensated for if
the goal is to keep the timing of principal cells constant even if those cells reside
in distant cortical modules. How this is done is not exactly clear. Below, I consider
a few possibilities.
If we know little about the types of interneurons, we know even less about the
relative frequency of cells in each interneuron class. As discussed above, the
numbers of neurons in each primary and secondary division vary considerably.
The most numerous interneuron types belong to the perisomatic control group,
followed by the dendrite-controlling groups, which innervate single or multiple
dendritic domains; the least numerous cells belong to the long-range interneuron
family. Independent of whether we subscribe to the “repeating module” concept
of the cortex or emphasize its small-world-like connectivity features, the relative
incidence of interneurons in the major divisions and the numerous subdivisions
are expected to have some mathematically definable relationship. It is highly unlikely
that proportions of interneurons in the different divisions (classes) with different
extents of axonal projections would scale proportionally in growing brains
for the same reasons discussed for the principal cells (Cycle 2). If a defined connectivity
is necessary for oscillatory timing of principal cells in a small rodent
brain, then how should the network be wired in the human brain so that the same
timing function is preserved?
78 RHYTHMS OF THE BRAIN
27. Changizi (2003) is an excellent source of the various scaling laws in the brain. It describes
physicomathematical models for numerous allometric (i.e., differential growth) relationships.
28. According to Lorente de Nó (1949), the morphology of cortical neurons becomes less uniform
and the number of nonpyramidal neurons increases as one ascends the phylogenetic scale. Yet, very
few comparative data are available to support or dispute this challenging claim.
The textbook recommendation for interneuron wiring is local connections, including
critical gap junctions among dendritically overlapping interneuron populations.
However, this creates a different but related problem: physically distant
neurons are not connected to each other, and this “disconnectedness” increases
monotonically with network size. Synaptic path length and, consequently, synaptic
and conduction delays become excessively long for synchronization in larger
networks. We need a mechanism that can compensate for the delay. The solution
for interneuron networks is the same as for principal cells: shortcuts. Such shortcuts
are accomplished by the long-range interneurons, which connect local interneurons
residing in different cortical regions. Now we can expect from the
small-world rules discussed in Cycle 2 that the fraction of long-range interneurons
in large brains will decrease substantially.27
The general conclusion that we can draw from the above discussion is that the
same physiological function in different-sized brains is supported by circuits with
different compositions of neuronal proportions and connectivity, which have to
be explored in the brains of each species to identify the particular wiring
schemes. Nevertheless, these quantitatively different architectures should have
some mathematically predictable relationships. This reasoning, of course, assumes
that all mammalian brains are built from essentially the same interneuron
types with similar connectivity principles. An alternative or complementary solution
would be to increase the diversity of interneuron types with the evolution of
the mammalian cortex. To date, there are no data available for such hypothetical
enrichment.28
In the last century, we went as far as we could to uncover and describe the microscopic
and macroscopic components of the brain. Progress over the past decade
brought us closer than ever to understanding the true nature of brain
topology. Now, it is time to see the functional consequences of this intricate
wiring. To achieve that, in the remaining Cycles I focus on the dynamics that take
place in the brain web.
Briefly . . .
In addition to principal cells, the cerebral cortex contains diverse classes of interneurons
that selectively and discriminately innervate various parts of principal
cells and each other. The hypothesized “goal” of the daunting connectionist
schemes of interneurons is to provide maximum functional complexity. Without
inhibition and dedicated interneurons, excitatory circuits cannot accomplish anything
useful. Interneurons provide autonomy and independence to neighboring
Diversity of Cortical Functions: Inhibition 79
principal cells but at the same time also offer useful temporal coordination. The
functional diversity of principal cells is enhanced by the domain-specific actions
of GABAergic interneurons, which can dynamically alter the qualities of the
principal cells. The balance between excitation and inhibition is often accomplished
by oscillations. Connections among interneurons, including electrical
gap junctions, are especially suitable for maintaining clocking actions. Thus, the
cerebral cortex is not only a complex system with complicated interactions
among identical constituents but also has developed a diverse system of components.
Cycle 4
Windows on the Brain
We shall not fail or falter; we shall not weaken or tire. . . . Give us the tools
and we will finish the job.
—Winston Churchill
80
The quote from Churchill sounds like an honest promise, but one might suspect
that it is just empty political rhetoric. Of course, if someone gives us the right
tools, we can succeed in anything. The usual problem is, however, that first one
has to invent those tools to succeed. To monitor the ever-changing patterns of
brain activity, neuroscientists need methods with sufficient spatial and temporal
resolution. The definition of “sufficient” in this context is a complex issue because
it varies with the level of analysis and expectation.
There are only a handful of tools at the neuroscientist’s disposal to monitor
brain activity without seriously interfering with it. Can we finish the job with
these tools alone? Maybe not, but for now, we have to live with them and believe
that we will not fail or falter. Each of the existing methods is a compromise between
spatial and temporal resolution. The desired temporal resolution is the operation
speed of neurons, that is, the millisecond scale. The desired spatial
resolution depends on the goal of the investigation and expands from the global
scale of the brain down to the spines of individual neurons. No current method is
capable of continuously zooming from the decimeter to the micrometer scale,
which is why several methods are being used, often in combination. Finding the
optimal level of resolution always depends on the question asked. This Cycle
summarizes the methods used for the exploration of brain activity, emphasizing
mostly those techniques that are most frequently used for monitoring oscillatory
Windows on the Brain 81
behavior of neuronal networks. If you have taken an introductory level class in
neurophysiological methods, feel free to skip it and come back if you need further
clarification.
EEG and Local Field Potential Recording Methods
Hans Berger’s noninvasive recording technique is still the most widespread
method used in clinical and psychological laboratories. The galvanometers are
now in museums; the voltage changes are now detected by highly sensitive ampli-
fiers and the traces are stored on fast computers. Recording EEG traces from a
few sites is sufficient to determine whether the brain is alive or dead or whether it
is sleeping or awake. However, deciphering the precise spatiotemporal changes in
the brain and how they are associated with the experience of, say, enjoying a Jackson
Pollock canvas or of remembering your first date is an entirely different challenge.
Increasing the number of recording sites is very useful only up to a limit,
because scalp electrodes placed too close together will sense pretty much the
same electrical fields without further enhancing spatial resolution (figure 4.1).
(Please note that the term “field” is often used differently by neurophysiologists
and physicists. For a neurophysiologist, the field or local field means extracellular
potential or EEG. For a physicist, field refers to a force defined at every point of
space generated by electric charge. The gradient of the field is the extracellular
potential.) In contrast to the excellent temporal resolution, scalp recording EEG
methods have serious spatial resolution problems that cannot be easily overcome,
for the reasons explained below.
With several recording sites on the scalp, a map of the brain’s electrical
changes can be constructed. The mapping technique was not invented by neuroscientists
or neurologists. Seismologists have used an identical method in their
effort to predict the time and place of destructive earthquakes. Our planet is covered
by thousands of seismograph stations. These stations transmit their data for
centralized real-time processing. The online processed data are disseminated to
concerned national and international agencies, which maintain an extensive,
global seismic database on earthquake parameters. Despite the eight-digit dollars
spent annually, the spatiotemporal resolution of earthquake predictions, as
we know, is far from adequate. The seismologists’ task is literally identical to
that of a neurologist who attempts to localize the source of an epileptic seizure
from scalp recordings. The source localization problem or, as engineers call it,
the “inverse problem” is the task of recovering the elements and location of the
neural field generators based on the spatially averaged activity detected by the
scalp electrodes. However, surface recordings provide only limited information
about the structures and neuron groups from which the hypersynchronous
epileptic activity emanates, and the inverse problem does not have a unique solution.
Localization of physiological, less synchronous patterns that generate
82 RHYTHMS OF THE BRAIN
Figure 4.1. Electrical activity of the cerebral cortex can be monitored by multiple electrodes
placed on the scalp (“geodesic” helmet, left). Better spatial resolution can be
achieved by subdural “grid” electrodes: intraoperative placement of the subdural grid after
craniotomy (top right) and the estimated electrode positions of the recording sites based on
the patient’s structural MRI (magnetic resonance imaging) scan acquired after the electrodes
were implanted (bottom right). Infant photo is courtesy of A. Benasich, Infancy
Studies Laboratory, Rutgers University; photo of grid electrodes is courtesy of R.T. Knight
and R. Canolty, University of California–Berkeley.
1. Nuñez (1998; 2000).
much smaller amplitude extracellular currents and fields is even more difficult.
Furthermore, numerous brain source configurations can produce identical electromagnetic
fields on the scalp, especially when measured at only a finite number
of electrode positions. The difficulty of source localization has to do with the
low resistivity of neuronal tissue to electrical current flow, the capacitive currents
produced by the lipid cell membranes, and the distorting and attenuating effects
of glia, blood vessels, pia, dura, skull, scalp muscles, and skin. As a result, the
EEG, recorded by a single electrode, is a spatially smoothed version of the local
field potentials under a scalp surface on the order of 10 cm2 and, under most conditions,
has little discernible relationship with the specific patterns of activity of
the neurons that generate it.1 The spatiotemporal integration problem of neuronal
activity is similar to statistical mechanics of physics in the sense that the specific
details of the neuronal interactions are replaced by the typical average behavior.
The EEG recorded from the scalp samples mostly the synaptic activity that
Windows on the Brain 83
2. Current density on the scalp (a measure of the volume conduction of current through the skull
and into the scalp, generated by the neurons) is sensitive mainly to superficial sources, with sensitivity
falling off at approximately r4 (r = distance from a current source or sink to the scalp surface; Pernier
et al., 1988) and insensitive to deep current sources in the brain. Scalp current density is the spatial derivative
of current flowing into and through the scalp.
3. Local field potentials are usually recorded by small-sized electrodes, e.g., a wire tip placed in
the depth of the brain, and reflect transmembrane activity of neurons in a more confined space than
does the scalp electroencephalogram (EEG). By definition, local field potential and EEG are synonymous
terms, but for historical reasons, EEG usually refers to scalp-recording field potentials. Activity
recorded by electrodes placed directly on the brain surface is called an electrocorticogram (ECoG).
Deep electrodes are most often used in patients with intractable epilepsies (Spencer, 1981; Engel,
2002).
occurs in the superficial layers of the cortex. The contribution of deeper layers is
scaled down substantially, whereas the contribution of neuronal activity from below
the cortex is, in most cases, virtually negligible. This “fish-eye lens” scaling
feature of the scalp EEG is the major theoretical limitation for improving its spatial
resolution.2
Depth Electrode and Subdural Grid Recordings
Precise localization of the anatomical structures that give rise to the physiological
abnormality is imperative in some clinical situations when the tissue has to be removed
surgically. In these difficult cases, several wire electrodes are implanted
into the suspected region, through which the locally generated extracellular field
potentials can be monitored, a method routinely used in animal experiments, as
well.3 A less invasive approach that yields localization effectiveness somewhere
between scalp recording and electrodes placed inside the brain is the subdural
grid electrode (figure 4.1). The grid, a flexible strip with 20–64 rectangularly
arranged electrodes, is introduced subdurally. Although inserting the grid by removing
a bone flap in the skull and placing it directly on the cortical (pial) surface
still requires surgery, both its implantation and removal are less invasive and less
risky than those of deep wire electrodes. The amplitude of the electrocorticogram
recorded by the grid electrodes is an order of magnitude larger than that of the
scalp EEG. The signals provide better spatial localization because the electrodes
integrate activity from a smaller brain area and are essentially free of muscle, eyemovement,
and other artifacts ubiquitously present in the scalp EEG of waking,
moving patients. Although these are superior features, the invasive grid electrode
recording technique cannot be used for research in healthy humans because of
ethical considerations. Fortunately, there is another method that can noninvasively
increase spatial resolution, while keeping the advantage of the outstanding
temporal resolution of the EEG. This technique monitors the magnetic rather than
the electric fields of the brain.
84 RHYTHMS OF THE BRAIN
4. Maxwell’s equations are a set of partial differential equations that describe and predict the behavior
of electromagnetic waves in free space, in dielectrics, and at conductor–dielectric boundaries.
Magnetic waves, generated by neurons, therefore can be sensed outside the brain and head. Unlike the
electric potential field, which is a scalar quantity, the magnetic field is a vector.
5. Brian D. Josephson was a graduate student at Cambridge University when he calculated in 1962
that electrical current would flow between two superconducting materials, even when separated by a
nonsuperconductor or insulator, known today as the Josephson junction. The discovery of the tunneling
phenomenon, or the “Josephson effect,” led to the design of SQUIDs. David Cohen (1968) detected
the first magnetic waves of the brain (occipital alpha oscillation).
6. Hämäläinen et al. (1993) provide detailed theoretical background for the MEG and SQUIDs and
compares EEG and MEG signal detection problems.
Magnetoencephalography
Luckily, Berger was a physician, not a physicist. Had he understood Maxwell’s
equations, he would not have started recording electricity from his son’s scalp in
his search for the carrier mechanisms of telepathy.4 Electricity needs a conductor
to propagate, and air is a poor conductor; thus brain currents do not go beyond the
scalp. However, voltage changes are accompanied by magnetic field changes. Because
the brain generates electromagnetic currents, they can be detected outside
the skull. The technical challenge one has to face, however, is dealing with the
very small magnitude of magnetic fields generated by neuronal activity. The magnetic
fields that emanate from the brain are only one hundred millionth to one billionth
of the strength of Earth’s magnetic field (or < 0.5 picotesla)! The sensor
that can detect such weak signals is known as a SQUID (superconducting quantum
interference device), a truly cool machine: it operates at–270°C. In essence,
it consists of a superconductive loop and two Josephson junctions.5 Liquid helium
in the SQUID chills the coils to superconducting temperatures. Like with EEG,
we need many sensors around the head to increase spatial resolution. The detector
coils are placed as close to each other as possible, forming a spherical
honeycomb-like pattern concentric with the head (figure 4.2).
A practical advantage of magnetoencephalography (MEG) is that no electrodes
need to be attached to the scalp because the magnetic field emerges from
the brain through the skull and the scalp without any distortion. The subject’s
head is simply fixed close to the surrounding coils. In contrast to the EEG, the
MEG signal reflects mostly intracellular currents. Partly for this reason, MEG and
EEG “see” different types of activity. For example, the radial sources that form
the best dipoles for scalp EEG are not well detected by MEG. Only currents that
have a component tangential to the surface of a spherically symmetric conductor
produce a magnetic field outside the scalp. This fact favors detection of activity
mainly from the fissures of the cortex. The spatial resolution of MEG is better
than that of the EEG (ideally less than a centimeter), mostly because, in contrast
to the EEG, the magnetic fields are not scattered and distorted by inhomogeneities
of the skull and scalp.6 Nevertheless, in practice, MEG source localization
is still not accurate because the model assumptions are overly simplified and
Windows on the Brain 85
Figure 4.2. Magnetoencephalography (MEG) can detect brain responses outside the skull.
The neuromagnetometer can record from numerous sites over the cerebral cortex (top panels).
The example shown at the bottom is a spontaneous oscillation in the superior temporal
lobe (lower right, arrow) at approximately 10 hertz (the auditory tau rhythm, part of the alpha
family; see Cycle 7). Auditory stimulation suppresses the rhythm. The MEG source of the tau
rhythm is described in Lehtela et al. (1997) and discussed in Cycle 7. Figure courtesy of R.
Hari and M. Seppä, Brain Research Unit, Helsinki University of Technology.
are not adequate to represent the complexity of the physics and physiology involved
in the human brain. Even under ideal conditions, the improved spatial resolution of
MEG is insufficient to obtain information about local circuits and layer-differential
effects in the cortex or about neuronal spikes, the necessary requirements for revealing
not only the locations but also the mechanisms of neuronal operations.
Origin of Local Field Potentials
The signals measured by EEG and MEG reflect the cooperative actions of neurons.
Not only neurons but also glia and even blood vessels can contribute to the
86 RHYTHMS OF THE BRAIN
mean field measured by EEG and MEG, but in order to keep things simple, let us
ignore the latter for the moment. The “mean field” measured outside the neurons
in the extracellular space simply reflects the “average” behavior of large numbers
of interacting neurons. The large degree of freedom—the essence of brain
activity—is thus replaced by the “typical” average behavior. The exact nature of
such cooperation is, of course, the million dollar question. Before attempting to
address this complex question, let us begin with a single neuron.
Neurons Communicate with Spikes
Neurons share the same characteristics and have the same parts as other cells in the
body, but they can pass messages to each other over long distances through their axonal
processes. Like virtually any cell in the body, neurons have a high concentration
of ions of potassium (K+) and chloride (Cl–
) inside and keep the sodium (Na+)
and calcium (Ca2+) ions outside. This arrangement produces a small battery that
maintains a voltage difference of –60 millivolts relative to the world outside of the
cell membrane. This ion separation is perhaps attributable to our single-cell ancestors
and where they came from: the sea. Given the high concentration of Na+ in seawater,
keeping Na+ outside the cell was a smart choice. However, when more
developed organisms migrated to land, they had to carry the sea with them to maintain
the same extracellular environment. For this purpose, the circulation of lymph
and blood developed. All our cells are constantly bathed in water, more precisely,
salt water. Each cell’s membrane is perforated by myriads of small pores, appropriately
called channels, through which ions can move in and out. Neurons can open
and close these ion channels very quickly, thereby altering the flux of ions and, as a
consequence, the voltage difference across the membrane. For example, the Na+
channel opening initially occurs linearly with time, with a consequent linear decrease
of the voltage difference between the inside and outside of the membrane:
the neuron depolarizes. However, after some critical amount of Na+ crosses the
membrane, something entirely novel occurs. At this critical threshold, Na+ influx
will facilitate the opening of additional Na+ channels, leading to an avalanche of
Na+ influx. This fast, strongly nonlinear event will depolarize the membrane so that
the inside becomes positive by about 20 millivolts, as if the battery was reversed
temporally. This fast depolarizing event is portrayed by the rising phase of the action
potential (figure 4.3). At this voltage level, the process stops mostly due to another
feature of the membrane, the voltage-dependent inactivation of Na+ channels.
Pumping all the excess Na+ out of the neuron is a lengthy process. To regain the
resting voltage across the membrane more rapidly, neurons opted for another strategy:
voltage dependence of K+ channel activity. As the action potential reaches its
peak, the voltage-dependent K+ channels are activated and quickly repolarize the
cell. This fast repolarization is the falling phase of the action potential (figure 4.3).
Thus, the positive charge created by the influx of Na+ is compensated for by the
quick efflux of equal charges carried by K+. This push-pull process, active during
the action potential, takes about a millisecond (absolute refractoriness) and limits
the maximum firing rate of the neuron. Because the action potential appeared as a
Windows on the Brain 87
100 m
20 mV
dendrite
K outflux Na
influx
soma
axon
1 ms
Figure 4.3. Fast action potentials propagate forward to the axon collaterals and backward
to the dendrites: action potential waveforms (left) recorded with patch pipettes (described
further below; see note 24) from the axon, soma, and dendrite in a layer 5 pyramidal neuron
(right). Note delays and the different kinetics of Na+ influx (rising phase of spike) and K+
outflux (falling phase) and the associated different waveforms of the voltage traces.
Reprinted, with permission, from Häusser et al. (2000).
7. Although failures may occasionally occur at junctions or in the terminals at higher frequencies,
the current view is that the low-frequency action potential travels to all presynaptic boutons.
short, large-amplitude event on the early chart recorders, investigators called the
action potential a “spike.” So when we refer to a spiking or firing neuron, what we
really mean is that the neuron gives rise to action potentials.
In contrast to the megahertz speed of computers, the speed of spike transmission
by neurons is limited to a maximum of a few hundred events per second.
Nevertheless, once an action potential is initiated, it can propagate through the entire
axon tree of the neuron and signal this event to all its downstream targets.7
88 RHYTHMS OF THE BRAIN
8. The quantitative description of the events associated with the action potential, by Alan Lloyd
Hodgkin and Andrew Fielding Huxley, remains among the most significant conceptual breakthroughs
in neuroscience. Their success story is also a reminder of the power of long-term collaboration between
people with different but overlapping expertise. For a quantitative description of the action potential,
see Johnston and Wu (1994).
Again, compared to the traveling velocity of electricity in computer circuits,
propagation of action potentials is quite sluggish at 0.5–50 meters per second, depending
on the caliber and insulation type of the axon cable.8 This slow transfer
of neuronal information by the traveling action potentials is the most important
limiting factor in the speed performance of neuronal networks.
Synaptic Potentials
Neurons are also good listeners, very much interested in what their upstream
peers have to say. At the contact point of each axon terminal or “bouton,” there is
a thin physical gap between the membrane of the axon terminal and the membrane
of the sensing neuron. This membrane–gap–membrane triad is called the
synapse (figure 4.4). The presynaptic terminal is specialized to release a chemical
substance, appropriately called a neurotransmitter, which then binds onto specialized
receptors on the postsynaptic side. All cortical pyramidal cells release glutamate,
which depolarizes and discharges the target neurons; therefore, glutamate is
referred to as an excitatory neurotransmitter. In contrast, GABA typically hyperpolarizes
the postsynaptic resting membrane, which is why GABA’s effect is
called inhibitory. Neurotransmitters exert their effect by binding to receptors that
Figure 4.4. Neurons communicate mainly with chemical synapses. Left: Neural tissue
with somata, dendrites, spines, and axons. Middle: An axon terminal (presynaptic, pre) in
synaptic contact with a target (postsynaptic, post) neuron. Neurotransmitter is packaged
into vesicles in the axon terminal. Upon arrival of an action potential and associated Ca2+
influx into the terminal, the vesicle empties its contents into the synaptic cleft, and the neurotransmitter
binds onto its receptors in the postsynaptic membrane. Right: Electron microscopic
picture of the synapse. Courtesy of T.F. Freund.
Windows on the Brain 89
9. Besides the major neurotransmitters glutamate and GABA, several other subcortical neurotransmitters
are known (see Johnston and Wu, 1994; see also Cycle 7).
10. A low-pass filter offers easy passage to low-frequency signals and difficult passage to highfrequency
signals because the capacitor’s impedance decreases with increasing frequency.
11. This is not necessarily the case under epileptic conditions, when neurons can synchronize
within the duration of action potentials. The synchronously discharging neurons create local fields,
known as compound or “population” spikes.
reside in the membrane of the postsynaptic neuron. When activated, the receptors
facilitate or suppress the kinetic activity of the Na+, K+, Cl–
, and Ca2+ channels so
that the membrane potential will deviate from the resting voltage (see figure
6.3).9 To define these respective events more clearly, we distinguish excitatory
postsynaptic potentials (EPSPs; or currents, EPSCs) from inhibitory postsynaptic
potentials (IPSPs; or currents, IPSCs). Compared to the fast action potentials,
membrane potential changes associated with EPSPs and IPSPs are several-fold
smaller in amplitude. However, they last for tens of milliseconds. This latter property
is critical for understanding the generation of EEG activity.
Extracellular Currents
For the transmembrane potential to change in a given neuron, there must be a
transmembrane current, that is, a flow of ions across the membrane. Opening of
membrane channels (or, more precisely, an increase in their open state probability)
allows transmembrane ion movement and is the source of ion flow in the extracellular
space. The local field potential (i.e., local mean field), recorded at any
given site in or outside the brain, reflects the linear sum of numerous overlapping
fields generated by current sources (current from the intracellular space to the extracellular
space) and sinks (current from the extracellular space to the intracellular
space) distributed along multiple cells (figure 4.5). The low resistance or “shunting”
effect of the extracellular fluid, the membranes of neurons, glia, and blood
vessels, and the slow movement of ions attenuate current propagation in the extraneuronal
space. Because the passive neuron acts as a capacitive low-pass filter,
this attenuation is quite discriminative: it affects fast-rising events, such as the extracellular
spikes, much more effectively than slowly undulating voltages.10 As a
result, the effects of postsynaptic potentials can propagate much farther in the extracellular
space than can spikes. Furthermore, because of their longer duration,
EPSPs and IPSPs have a much higher chance to occur in a temporally overlapping
manner than do the very brief action potentials. Finally, EPSPs and IPSPs are displayed
by many more neurons than are spikes because only a very small minority
of neurons reach the spike threshold at any instant in time. For these reasons, the
contribution of action potentials to the local field and especially to the scalp EEG
is negligible.11
Excitatory currents, involving Na+ or Ca2+ ions, flow inwardly at an excitatory
synapse (i.e., from the activated postsynaptic site to the other parts of the cell) and
outwardly away from it. The passive outward current far away from the synapse is
referred to as a return current from the intracellular milieu to the extracellular
90 RHYTHMS OF THE BRAIN
Figure 4.5. Generation of extracellular field potentials. Left: Spontaneously occurring
field potential (sharp wave) recorded simultaneously in various layers of the hippocampus
(CA1–dentate gyrus axis). The traces represent averages of 40 events. Middle: Currentsource
density map, constructed from the field potentials. Interpretation of the current
sinks (s) and sources (so) is on the basis of anatomical connectivity, representing different
domains of parallel-organized pyramidal cells and granule cells. Active currents are indicated
on the right, and passive return (re) currents on the left, of the pyramidal neuron. The
sinks in the dendritic layers are caused primarily by excitation from the upstream CA3 pyramidal
cells, whereas the source around the soma reflects mainly inhibition, mediated by
basket interneurons. Iso, isoelectric (neutral) state.
12. This “classical” description of the origin of extracellular fields must be supplemented by the
recent findings about the active properties of neurons (see Cycle 8; Llinás, 1988). Subthreshold oscillations,
afterpotentials, Ca2+ spikes, and other intrinsic events also produce relatively long-lasting
transmembrane events. The contribution of these nonsynaptic events to the local field potential can often
be more important than the contribution of synaptic events (Buzsáki et al., 2003b).
space. Inhibitory loop currents, involving Cl– or K+ ions, flow in the opposite direction.
Viewed from the perspective of the extracellular space, membrane areas
where current flows into or out of the cells are termed sinks or sources, respectively.
The current flowing across the external resistance of the extraneuronal
space sums with the loop currents of neighboring neurons to constitute the local
mean field or local field potential (figure 4.5). In short, extracellular fields arise
because the slow EPSPs and IPSPs allow for the temporal summation of currents
of relatively synchronously activated neurons.12
Depending on the size and placement of the extracellular electrode, the volume
of neurons that contributes to the measured signal varies substantially. With
very fine electrodes, the local field potentials reflect the synaptic activity of tens
to perhaps thousands of nearby neurons only. Local field potentials are thus the
electric fields that reflect a weighted average of input signals on the dendrites and
cell bodies of neurons in the vicinity of the electrode. If the electrode is small
Windows on the Brain 91
13. Current density is is the current entering a volume of extracellular space, divided by the volume.
The current flow between two sites (e.g., between recording electrodes 1 and 2 and between electrodes
2 and 3 in the example) can be calculated from the voltage difference and resistance using
Ohm’s law. The difference between these currents (i.e., the spatial derivative) is the current density.
More precisely, the current density is a vector, reflecting the rate of current flow in a given direction
through the unit surface or volume (measured in amperes per square meter for a surface and amperes
per cubic meter for a volume). Current density depends on both the electric field strength and the conductivity
(σ) of the brain. The conductance is a factor of both conductivity and the shape of volume.
Conductivity is inversely proportional to resistivity. The average resistivity of white matter is ∼ 700 Ω.
cm, and that of gray matter is ∼ 300 Ω.cm. The proportion of fibers therefore significantly affects tissue
resistivity. For a thorough theoretical discussion of the current density method, I recommend
Mitzdorf (1985) and Nicholson and Freeman (1975).
enough and placed close to the cell bodies of neurons, extracellular spikes can
also be recorded. Therefore, in such a small volume of neuronal tissue, one often
finds a statistical relationship between local field potentials, reflecting mostly input
signals (EPSPs and IPSPs), and the spike outputs of neurons. The reliability
of such relationship, however, progressively decreases with increasing the electrode
size, by lumping together electric fields from increasingly larger numbers
of neurons. This is why the scalp EEG, a spatially smoothed version of the local
field potential at numerous contiguous sites, has a relatively poor relationship
with spiking activity of individual neurons.
In architecturally regular regions of the brain, such as the neocortex, the locations
of the extracellular currents reflect the geometry of the inputs. Using several
microelectrodes with regular distance from each other, one can calculate the density
of the local currents from the simultaneously measured voltages, provided
that information is available about the conductance of the tissue. Consider a distant
current source relative to three equally spaced recording sites. Each electrode
will measure some contribution of the field (due to the passive return currents that
pass through the extracellular space) from the distant source. The voltage difference
between two adjacent electrodes can determine the voltage gradient, that is,
how fast the field attenuates with distance from the current source. Because the
source is outside the area covered by the electrodes, the voltage gradient will be
the same between electrodes 1 and 2 and between electrodes 2 and 3. Taking the
difference between the voltage gradients, we get a value of zero, an indication that
the measured field did not arise from local activity but was volume-conducted
from elsewhere. In contrast, if the three electrodes span across a synchronously
active afferent pathway, the voltage gradients will be unequal and their difference
will be large, indicating the local origin of the current. By placing more microelectrodes
closer to each other, we can more precisely determine the maximum
current density and therefore the exact location of the maximum current flow.13
Unfortunately, from measuring the local current density alone, we have no
way of telling whether, for example, an outward current close to the cell body
layer is due to active inhibitory synaptic currents or if it reflects the passive return
loop current of active excitatory currents produced in the dendrites. Without additional
information that can clarify the nature of the current flow, the anatomical
92 RHYTHMS OF THE BRAIN
14. Spike occurrences of in the vicinity of the cell body of the neurons reliably reflect their output
messages. Unfortunately, no reliable methods exist to monitor all individual inputs to a single neuron
simultaneously. Inputs can be estimated only by recording the local field potentials that reflect the spatially
averaged activity of many neurons and inferring indirectly the mean input. Another, equally dif-
ficult approach is to monitor the spike output of the afferent neurons to the chosen recipient neuron
and infer the input configurations from their spiking.
source remains ambiguous. The missing information may be obtained by simultaneous
intracellular recording from representative neurons that are part of the population
responsible for the generation of the local current. Alternatively, one can
record extracellularly from identified pyramidal cells and interneurons and use
the indirect spike-field correlations to determine whether, for example, a local
current is an active, hyperpolarizing current or a passive, return current from a
more distant depolarizing event. Taking these extra steps is worthwhile. The reward
one obtains by pinning down the currents is crucial information about the
anatomical source of the input to those same neurons whose output (i.e., spiking)
activity is simultaneously monitored. Once information about both the input and
output of a small collection of neurons working together becomes available, one
may begin to understand the transformation rules governing their cooperative action.
This approach is the next best thing to the ideal condition when all inputs
(synapses) and the output of each cell could be monitored simultaneously and
continuously.14
Functional Magnetic Resonance Imaging
Currently, the best-known noninvasive procedure for the functional investigation
of the human brain is magnetic resonance imaging (MRI). The method is based
on the detection and analysis of magnetic resonance energy from specific points
in a volume of tissue. The MRI technique provides far better images than those
the traditional X-ray and other scanning technologies. Hydrogen atoms of water
represent tiny magnetic dipoles, which can align in an orderly way when placed
inside of a very strong magnetic field. In practice, a short pulse of RF energy perturbs
these tiny magnets from their preferred alignment. As they subsequently return
to their original position, they give off small amounts of energy that can be
detected and amplified with a “receiver coil” placed directly around the head. The
injection of electromagnetic energy into a single plane is used to produce a slice
through the brain volume. To produce consecutive brain slices, the head is advanced
in small increments. Because gray matter and white matter contain different
amounts of water, this difference generates a contrast between the surface of
the neocortex and the underlying white matter and other areas of the brain that
can be used to provide a detailed image of the brain. However, while the MRI
method offers exquisite details about the structure of the brain, it does not tell us
anything about neuronal activity.
As previously mentioned, active neurons consume a lot of energy, and in areas
Windows on the Brain 93
15. During the late 1980s, Seiji Ogawa, then at Bell Labs in Murray Hill, New Jersey, noted that
cortical blood vessels became more visible as blood oxygen was lowered. From these initial observations,
he concluded that the local magnetic field inhomogeneities can be used to assess neuronal activity,
and termed his invention the blood-oxygenation-level–dependent (BOLD) method (Ogawa et
al., 1990). For a brief discussion on the complex origin of BOLD, I suggest Logothetis (2003), Logothetis
et al. (2001), and Raichle (2003).
with high neuronal activity this results in a large difference between the concentration
of the oxygenated hemoglobin in the arterial blood and the deoxygenated
hemoglobin in the venous outflow. These local magnetic-field inhomogeneities
can be assessed by the BOLD (blood-oxygenation-level–dependent) method.
Functional MRI (fMRI), which uses the BOLD method, can measure neuronal
activity indirectly.15 Because of the unprecedented details of localized changes in
the brain in response to various challenges and perturbations, the fMRI method
has become the leading tool in cognitive science research. Nevertheless, as with
any technique, fMRI has its limitations. The first limitation has to do with the
general statement that “fMRI measures neuronal activity.” Neuronal activity has
numerous components, including intrinsic oscillations, EPSPs, IPSPs both in
principal cells and in inhibitory interneurons, action potential generation and
propagation along the axon, and release, binding, reuptake, and reprocessing of
the released neurotransmitter. Which of these processes, alone or in combination,
are responsible for the changes in BOLD has yet to be worked out. Without such
crucial information, it is not possible to conclude whether an increase in BOLD
results from increased firing of principal cells or interneurons or increased release
of neurotransmitter from afferents whose cell bodies are outside the area with increased
BOLD signal.
The second problem arises from the neurophysiological observations that numerous
brain operations are brought about by changing the firing patterns of neurons
without any change in the rate of postsynaptic potentials or alteration of
neuronal firing rates (I provide some examples in Cycles 8, 9, and 12). For example,
recognition or recall of the correct and incorrect information may use different
sets of neurons but engages those neurons with the same magnitude of
activity. Thus, fundamentally different cognitive operations in the same structures
can be generated with the same amount of energy, with no expected change in
BOLD. This reverse engineering problem is, of course, identical to that of the
EEG and MEG. Thus, with the exception of significantly improved spatial resolution,
one cannot expect more from fMRI than from EEG measurements.
The third technical drawback of fMRI is its slow temporal resolution. Not only
is the blood-flow response delayed about half a second after neuronal activation,
but also the second-scale temporal resolution of the BOLD imaging method is excessively
long for assessing spatiotemporal evolution of neuronal activity across
brain domains. As discussed in Cycle 2, activity can get from any structure to just
about any other structure in the brain by crossing just five to six synapses within a
second. Even if only a few areas show increased BOLD activity, we have no
knowledge about the temporal sequence of their activation, a critical issue for
94 RHYTHMS OF THE BRAIN
understanding how the information is processed. Understanding the neuronal
mechanisms that give rise to overt and cognitive actions requires method(s) at the
temporal resolution of behavior.
Positron Emission Tomography
Another important research tool for visualizing brain function is positron emission
tomography (PET). A major advantage of PET is that it provides information
about the use and binding of specific chemicals, drugs, and neurotransmitters in
the brain. To obtain a PET scan, the subject either inhales or receives an injection
of a very small amount of a radiolabeled compound, which then accumulates in
the brain. As the radioactive atoms in the compound decay, they release positively
charged positrons. When a positron collides with a negatively charged electron,
they are both annihilated, and two photons are emitted. The photons move in opposite
directions and are detected by the sensor ring of the PET scanner. Reconstruction
of the three-dimensional paths of the particles provides information
about the maximum accumulation or metabolism of the radiolabeled isotope.
Both the spatial and temporal resolutions of PET are inferior to fMRI.
Let me pause here to add a few important details regarding all of these advanced
imaging methods. A single MEG, PET, or fMRI device weighs several
tons. Because the subject’s head must be immobilized for brain scanning, these
methods are not practical for the examination of behavior-generated brain
changes in the most frequently used small laboratory animals, such as rats and
mice. More important, even the combined, simultaneous application of these
methods falls short of the goal of explaining how neurons and neuronal assemblies
make sense of the world, generate ideas and goals, and create appropriate responses
in a changing environment. In the brain, specific behaviors emerge from
the interaction of neurons and neuronal pools. Although EEG, MEG, fMRI, PET,
and related methods have opened new windows on brain function, in the end all
these indirect observations need to be reconverted into a common currency—the
format of neuronal spike trains—to understand the brain’s control of behavior.
Increasing Spatial and Temporal Resolution:
Optic Methods
To date, the best spatial resolution of neuronal activity is provided by optical
methods. By viewing through a microscope, light intensity or color changes can
be monitored at the micrometer scale, and at the same time, large twodimensional
areas can be observed, just like watching a movie screen. The trick is
to extract functional information from the optically detected signals. The most
prominent pioneer in this field has been Amiram Grinvald of the Weizmann Institute
of Science in Rehovot, Israel. Working first with invertebrates and later in the
Windows on the Brain 95
16. Hemoglobin is a protein that binds oxygen. Cytochromes are energy-producing enzymes in the
inner mitochondrial membrane that catalyze the reaction between ferrocytochrome c and oxygen to
yield ferricytochrome c and water. It is associated with the pumping of protons and the resultant phosphorylation
of ADP to ATP, a molecule in great demand for energy. The high metabolic rate of neurons
explains their strong cytochrome activity. Fast-firing interneurons have a particularly high
density of cytochromes (Gulyás et al., 2006); therefore, they may bias the optical images.
17. For an overview of the fast imaging methods with voltage-sensitive dyes, see Grinvald and
Hildesheim (2004). In principle, optical fibers can be lowered into the depth of the brain. However, the
spatial coverage of this invasive modification is limited.
18. Winfried Denk, then a graduate student at Cornell University in New York, James Strickler,
and their adviser, Watt W. Webb, constructed the first 2-PLSM in 1990 (Denk et al., 1990).
monkey visual cortex, Grinvald noticed that neuronal activity affects the optical
properties of brain tissue, which can be conveniently monitored by photondetecting
arrays or sensitive cameras. His method is known as intrinsic optical
imaging, because it is based on the light-reflecting/absorbing properties of the intact
brain tissue. All that is needed is a very sensitive, fast camera to be able to
watch the brain in action. Unfortunately, interpretation of the obtained images in
terms of neuronal function is even more difficult than in the case of fMRI. The
potential sources for these activity-dependent intrinsic signals are numerous and
include changes in the physical properties of the tissue itself, which affect light
scattering, or changes in the absorption, fluorescence, or other optical properties
of various molecules having significant absorption or fluorescence, for example,
hemoglobin or cytochromes.16
The temporal resolution of the intrinsic imaging method can be significantly
improved by using compounds whose optical properties can be altered by some
brain mechanisms. For example, voltage-sensitive dyes bind to the external surface
of the neuronal membrane and act as molecular transducers to transform
changes in membrane potential into optical signals. Optical imaging with voltagesensitive
dyes and fast photodetecting devices permit the visualization of neuronal
activity with improved temporal resolution and a spatial resolution of
approximately 100 micrometers. This method combines the advantageous features
of surface local field potential recordings with high spatial resolution. There
are a few methodological caveats, however. The dye has to be applied physically
to the surface of the brain, making prolonged and repeated observations difficult.
Single cells cannot be identified, and more critically, input and output actions of
the neurons cannot be separated; thus, their contribution to the transfer of information
can be inferred by indirect means only or from a combination with other
methods. Finally, because the optical method works much like our video cameras,
it allows for the observation of surface events only, and it is hard to figure out
what happens inside the neocortex or in deeper brain structures.17
Penetration into deeper layers of the neocortex is possible with another innovation,
known as two-photon or multiphoton laser scanning microscopy (2-PLSM
or m-PLSM).18 Only three things are needed for this powerful method to work:
extremely powerful laser pulses in the 700–900 nanometer range (deep red to infrared),
molecules that change their fluorescence relevant to some physiological
96 RHYTHMS OF THE BRAIN
19. After moving to the Bell Laboratories, Denk and his colleagues David Tank, Karel Svoboda,
and Rafael Yuste provided images of neurons from the living brain with details that rivaled those visualized
in fixed tissue and coupled these detailed images to brain function (Denk et al., 1996). Recognizing
that the most meaningful test of any hypothesized brain mechanism is behavior, they
constructed a miniaturized prototype of the 2-PLSM that can potentially be carried by a small animal
(Helmchen et al., 2001).
activity, and the ability of the microscope to collect the emitted fluorescence
photons for producing a three-dimensional image. Very high energies are needed
for two- or multiphoton interactions with the fluorescing target. When this
occurs, the individual energies of the photons combine, and the cumulative
effect is the equivalent of delivering one photon with twice the energy (in the
case of two-photon excitation) or three times the energy (in three-photon excitation).
High-wattage lasers can easily fry the brain instantaneously. To avoid such
an undesirable effect, the laser beams are pulsed so that only very short, 100 femtosecond
long pulses penetrate the brain. The scanning beam is a moving point,
like the cathode ray in the TV screen tube; therefore, the brain targets are affected
only at the time the beam moves across them. The 2-PLSM produces highresolution,
three-dimensional pictures of tissues with minimal damage to living
cells.19
Most current functional measurements with 2-PLSM investigate intracellular
calcium changes, simply because effective fluorescent sensors are available for
this ion. Methods for the direct detection of action potentials and other functional
indices are being developed. The optical imaging methods will fully reach their
potential when combined with the rapidly evolving tools of molecular biology for
the creation of function-sensing fluorescent markers. Some further practical
problems, such as the trade-off between temporal and spatial resolution, can be
addressed and perhaps resolved. However, obtaining images in deep layers of the
neocortex or structures below the cortex remains a challenge even in small animals.
In the meantime, alternative methods are needed to monitor the cooperative
action of individual neurons.
Recordings from Single Neurons In Vitro
Neurons are complex devices. Understanding the biophysical properties of individual
neurons would greatly enhance understanding their collective behavior in
networks. Characterization of individual neurons is especially critical in brain regions
built from a variety of different neuron types. Most of our knowledge about
the biophysical properties of neurons is derived from experiments carried out in
brain slice preparations in vitro. Although the brain slice method compromises
brain circuits, it provides unprecedented spatial resolution, precision, and pharmacological
specificity for the examination of the biophysical and molecular properties
of the cell membrane. Brain slices allow recording from local neural circuits,
Windows on the Brain 97
20. Brain slices are used for a wide variety of studies, including synaptic plasticity and development,
network oscillations, and intrinsic and synaptic properties of anatomically defined neurons. The
in vitro slice preparation was introduced by Yamamoto and McIlwain (1966). Soon after it was
adopted for the physiological examination of the hippocampus in Per Andersen’s laboratory in the Institute
of Physiology, Oslo, Norway in the 1970s, it became the method of choice for biophysical and
pharmacological investigation of single neurons (see Skede and Westgaard 1971).
21. Khalilov et al. (1997). In addition to studying single neurons, brain slices have been used extensively
to study the emergence of network oscillation. A variety of oscillatory patterns, reminiscent
of those in the intact brain, have been replicated in brain slices despite the fact that in these reduced
preparations only a small fraction of the in vivo network is present. Do these in vitro models faithfully
represent the rhythms they intend to mimic? If so, the model is of great value, because the reduced
preparations exclude a large number of variables that are uncontrollable in the intact brain and allow
for systematic changes of various parameters that are critical for the emergence, maintenance, and termination
of the oscillation. Reduction of the parameter space, in turn, allows for the construction of
computer models for the identification of the necessary and sufficient conditions underlying various
aspects of the oscillations (Traub et al., 1999; Destexhe and Sejnowski, 2001, 2003). The final and
most important step in this “reverse engineering” strategy is the comparison of the in vitro and in silico
(i.e., computational modeling) engineered rhythms with those of the intact brain. It is this stage
that should address the important question of whether the evoking conditions in the reduced system
are in fact present in the intact brain and to identify the similar and dissimilar aspects of the observed
and created oscillations. This process should also identify features of the oscillation that cannot be reproduced
in the model and should thus point to the need for larger circuits and more complex interactions
than are offered by the model. Unfortunately, in vivo and in vitro experiments and computational
modeling are rarely done in the same laboratory. As a result, models too often claim too much. On the
other hand, critical details are often not accessible through the in vivo approach. It is fair to say that, to
date, oscillations that have been reproduced and studied with excruciating details in vitro and in models
are best understood.
with the advantages of mechanical stability, direct visualization of neurons, and
the experimenter’s control over the extracellular environment.20 Depending on
the age of the animal, the thinly cut sections of the brain, placed in a humidified,
temperature-controlled dish and perfused with oxygenated cerebrospinal fluid,
can be kept alive for several hours. Using a microscope and an infrared camera,
the outlines of individual neurons can be visualized. In case of very young animals,
whole pieces of brain structures, for example, the hippocampus, can be kept
alive in vitro. Various drugs and electrolytes can be perfused or applied locally
under visual control.21
The popularity of the brain slice method was catalyzed by another groundbreaking
innovation, the patch-clamp technique, introduced by Erwin Neher and
Bert Sakmann of the Max Planck Institute for Biophysical Chemistry in Göttingen,
Germany. The key invention in patch-clamp recording is the use of a pipette
with a finely polished end. The pipette can be attached gently to the cell membrane,
and by applying negative pressure through the pipette, a piece of the cell
membrane, the patch, is “sucked” into the pipette. The result is that the membrane
attached to the pipette becomes mechanically and electrically isolated (“sealed”)
from the surrounding extracellular fluid. By applying a short pulse of low pressure
through the pipette, the patch can be broken and a direct junction between the
98 RHYTHMS OF THE BRAIN
22. The membrane can be penetrated by sharp glass electrodes, as well. This sharp electrode approach
has been used successfully for intracellular recordings of neurons for many years. However,
because its tiny tip cannot be easily visualized under the microscope, the sharp electrode methods became
less popular in in vitro experiments.
23. The patch-clamp method in fact refers to four different methods. (1) The most popular version
is the whole-cell method, typically patching the cell body. A major advantage of the whole-cell
method is the very low access resistance between the electrode content and the cytosol. This allows
“clamping” the membrane voltage to any arbitrary value, e.g., reversal potential of chloride or
sodium, and measuring the current flowing through the membrane. (2) The “cell-attached” method is
basically an extracellular method. A difference from other extracellular methods is that the tip of an
electrode is attached tightly to the membrane and the inside of the pipette is isolated from the extracellular
environment electrically (a “gigaohm seal” is established). This feature prevents signals from
other neurons from interfering with the activity of the recorded neuron. The polished pipette method
is also useful to investigate ion channels in excised patches of the membrane without the influence
from the rest of the neuron. (3) In the inside-out configuration, the outer membrane surface is facing
the pipette’s orifice whereas (4) in the outside-out configuration it is the inner membrane surface. Prior
to the patch-clamp method, neurons had been impaled with a sharp glass electrode successfully for
many years. The disadvantages of glass pipettes are their small size and the high resistance of their
tips, which makes delivering chemicals and large currents more difficult.
24. The tungsten extracellular electrode method was introduced by Hubel (1957) for the exploration
of single neuronal responses in the visual cortex. The single-cell recording method has generated
a wealth of information about a wide range of issues, including sensory representation,
short-term memory, and motor organization.
inside of the neuron and the electrolyte solution in the pipette is established.22
Any current waveform can be applied, and relatively large molecules can be
“washed” into the neuron through the pipette. Alternatively, a piece of a membrane,
sealed to the pipette tip, can be torn away, and the channels inside the
membrane piece can be studied electrically and pharmacologically. Patch-clamp
experiments performed in the in vitro slice preparation have provided unprecedented
details about the active properties of neurons and important insights into
the mechanisms of network oscillations.23
Extracellular Recordings from Single Neurons
Because action potentials produce large transmembrane potentials in the vicinity
of the cell body, the occurrence of the spike can be sensed by a conductive microelectrode
positioned near the cell body of the neuron by means of a precision mechanical
drive. The voltage-sensing microelectrode, in essence, is a very sharp
insect pin, insulated except for the last few micrometers of the tip. The closer the
recording tip is to the neuron, the louder one can hear the action potentials, provided
that the amplified voltage is connected to a loudspeaker. With some maneuvering
by a mechanical micromanipulator device, the signal can be maximized so
that one neuron’s “voice” stands out from the others, a procedure called cell “isolation.”24
If other active (spiking) neurons are in the vicinity of the tip, the electrode
records from all of them. Because neurons of the same class generate
virtually identical action potentials, the only way to identify a given neuron from
Windows on the Brain 99
25. Willem Einthoven, whose string galvanometer was used to detect EEG signals by Berger,
worked out the first diagnostic rules of heart signals on the basis of triangulation. The Einthoven triangle
is an imaginary equilateral triangle with the heart at its center, its equal sides representing the
three standard limb leads of the electrocardiogram (see figure 4.6, top right). In the electrocardiogram,
at any given instant the potential of any wave between two limbs is equal to the sum of the potentials
in leads obtained from the other limbs.
26. McNaughton et al. (1983a) and Gray et al. (1995). The tetrode catapulted into its present fame
after Wilson and McNaughton (1993) recorded from more than 100 neurons in a freely moving rat, using
12 movable tetrodes.
extracellularly recorded spikes is to move the electrode tip closer to its body (< 20
micrometers in cortex) than to any other neuron. The much larger spike from the
closest cell, relative to the spikes of more distant neurons, is often sufficient to reliably
monitor the output of a single cell. However, being very close to a neuronal
membrane with a sharp tip can be dangerous. Very small movements of the brain,
due to pulsation of the vessels, breathing-related shifts, or head position changes
can affect the relationship between the electrode tip and the neuron. The neuron is
easily injured, and perturbation of its immediate environment can affect its firing
pattern. To record from another neuron with certainty, yet another electrode is
needed.
Triangulation of Neurons by Tetrodes
Triangulation of biological voltage sources began with the studies of the heart,
and the triangulation method has remained a routine method in the evaluation of
the electrocardiogram (EKG). The heart is massively connected torus of muscle
cells and produces a huge electrical signal (in the millivolt range) compared to the
brain. Created by placing just four electrodes on the left foot, right and left arms,
and chest, the heart “tetrode” is a pretty efficient routine clinical tool for the localization
of the anatomical sources of the components of the EKG (figure 4.6).
For more precise localization, many more electrodes are used.25 The triangulation
method, of course, should work for the localization of any stationary dipole, including
action potentials generated by neurons. The idea of triangulation for neuron
separation was first introduced by John O’Keefe and Bruce McNaughton, at
University College, London. Their first sensor consisted of two 25-micrometerlong
insulated wires twisted together, fittingly called the “stereotrode,” followed
by the four-wire version called a “tetrode” (figure 4.6, bottom).26 A typical tetrode
consists of four thin wires (12–15 micrometers in diameter each) glued together
in a bundle. With only one electrode tip, signals from many neurons that are located
at the same distance from the tip in a sphere provide similar amplitude
spikes, making single-cell isolation difficult. Thus, we may hear and see many
neurons, but we cannot tell them apart. With two closely spaced electrodes, the
ambiguity can be decreased to neurons in a plane, and with three electrodes to
neurons sitting in a line. If a fourth electrode is placed in different plane from the
Figure 4.6. Source localization by triangulation. For determining the heart’s electrical
axis, voltage measurements are made between the right and left arms, right arm and left
leg, and left arm and left leg (top right). Photograph (top left) illustrates Willem
Einthoven’s “electrodes”: the subject places an arm and a leg in salt water connected to a
galvanometer. From the voltage deflections in each measurement, the voltage vector can be
calculated. Bottom: Triangulation of the three-dimensional position of neurons by
“tetrode” measurements. The voltage differences between the wires of the tetrodes of the
recorded spikes from individual neurons allow the calculation of the unique position of
each neuron. Modified, with permission, from Buzsáki (2004).
100
Windows on the Brain 101
27. Henze et al. (2000a) determined the relationship between the distance of the extracellular electrode
from the spiking neuron and the amplitude of the recorded spikes by monitoring the same neurons
both intracellularly and extracellularly in the intact hippocampus. For modeling extracellular
spike waveforms, see Gold et al. (2005). A concise overview of the extracellular recording methods is
Nádasdy et al. (1998).
other three, the spatial position of each neuron in the line, in principle, can be separated
by triangulation.
Wire tetrodes have numerous advantages over sharp-tip single electrodes.
First, they provide recording stability. The thin wires are flexible and can move
together with the brain to some extent. This is why recordings of neurons in
deeper structures are more stable than those of neurons in, for example, the superficial
layers of the cortex. Because the recording tip need not be placed in the
immediate vicinity of the neuron, small movements of the tetrode are not as detrimental
as would be the case of a sharp tip touching the neuron. Tetrodes are especially
useful in areas with a high packing density of neurons, where isolation of
individual neurons from nearby peers is difficult with single wires. Under ideal
conditions, a tetrode can record up to 20 well-isolated neurons.
Cortical pyramidal cells generate extracellular fields that flow mostly parallel
to their somadendritic axis. For this reason, the action potentials can be detected
several hundred micrometers from the soma, in the proximity of thick apical dendrites.
The lateral spread of current is more restricted. Nevertheless, tetrodes can
“hear” pyramidal cells as far away as 140 micrometers lateral to the cell body, although
the extracellular spike amplitude decreases rapidly as a function of distance
from the neuron. A cylinder with a radius of 140 micrometers contains
approximately 1,000 neurons in the rat cortex, which is therefore the upper limit
of the theoretically recordable cells by a single electrode. Yet, in practice, only a
small fraction of these neurons can be reliably separated (up to 20 neurons under
ideal conditions).27 The remaining neurons may be damaged by the blunt end of
the closely spaced wires or remain undetected with currently available spikesorting
algorithms. Thus, there is a large gap between the number of routinely
recorded and theoretically recordable neurons. To monitor another dozen or so
neurons, another tetrode is needed. Because inserting wires in the brain is an invasive
procedure, recording from large numbers of neurons is possible only at the
expense of extensive cell damage.
High-Density Recordings with Silicon Probes
An ideal recording electrode has a very small volume, so that tissue injury is minimized.
On the other hand, recording from many neurons with wire electrodes requires
large numbers of wires with consequent tissue damage. Obviously, these
competing requirements are difficult to meet. A wire electrode has only one useful
site, the conductive tip; the rest is just conduit and inconvenient bulk. To increase
the number of useful recording sites without increasing the volume of the
102 RHYTHMS OF THE BRAIN
Figure 4.7. Functional connectivity within local microcircuits of the somatosensory cortex
of the rat. Synaptic connections between participating pyramidal cells (triangles) and
putative interneurons (circles) can be determined by their temporal relationship. For example,
decreased discharge of a partner neuron immediately after the spike of the reference
cell (time zero in the upper white histogram) reveals the inhibitory nature of the reference
neuron. Conversely, a consistent, short-latency discharge of the partner neuron after the
reference spike (lower histogram) indicates the excitatory nature of the reference cell.
Modified, with permission, from Barthó et al. (2004).
28. Wise and Najafi (1991). A major advantage of silicon probes is that with integrated chip technology
virtually any two- and even three-dimensional arrays can be fabricated, which is a highly desirable
feature for the exploration of local networks. In addition to recording from multiple sites,
silicon electrodes can use on-chip amplification, filtering, and time-division multiplexing as well as
programmed microstimulation through the recording sites and, potentially, real-time signal processing
(Olsson et al., 2005). Such a brain–chip interface may allow for reciprocal interactions with the brain,
paving the way for fully implantable neural prosthetic devices.
electrode, Kensall Wise at the University of Michigan devised multisite recording
probes, using silicon “chip” technology. These microelectromechanical system
(MEMS)–based recording devices can reduce the technical limitations inherent in
wire electrodes because with the same amount of tissue displacement, the number
of monitoring sites can be substantially increased. These silicon devices share the
advantages of tetrode recording principles, yet they are substantially smaller in
size. At the current level of development, multishank silicon probes can record
from as many as 100 well-separated neurons. Importantly, the geometrically precise
distribution of the recording sites also allows for determination of the spatial
relationship and the functional connectedness of the isolated single neurons (figure
4.7). This feature is a prerequisite for studying the spatiotemporal representation
and transformation of inputs by neuronal ensembles.28
Windows on the Brain 103
29. The spike amplitude variation is most substantial during complex spike-burst production,
when the amplitude of the extracellular spikes may decrease as much as 80 percent, associated with
changes in other waveform parameters.
30. The amplitude and waveform variability of the extracellularly recorded spike is the major
cause of unit isolation errors. Triangulation methods visually analyze two-dimensionally projected
datasets one at a time. With multisite-recorded data, successive comparisons of the various possible
projections generate cumulative errors of human judgment. Cumulative human errors can be reduced
Isolation and Identification of Neurons by
Extracellular Signatures
An indispensable step in spike-train analysis is the isolation of single neurons on
the basis of extracellular features. Spike-sorting methods fall into two broad
classes. The first class attempts to separate spikes on the basis of amplitude and
waveform variation, on the assumption that neighboring neurons generate distinct
spike features. This assumption is difficult to justify in most cases because similar
neurons at similar distances from the recording electrode tip generate nearly identical
waveforms. As a result, these neurons may be inadvertently combined as if
the spikes were generated from a single neuron. The converse problem also occurs:
the same neuron can generate different waveforms, depending on firing
rates, the magnitude of somadendritic propagation of spikes, and activation of
various channels in different states. The consequence of these wave shape variations
is that spikes generated by the same neuron under different conditions will
be classified by the wave shape discrimination method as if the spikes with different
forms emanated from different cells.
The second general approach, the triangulation method discussed above, is
based on the tacit assumption that the extracellularly recorded spikes emanate
from point sources rather than the complex geometry of neurons. This is obviously
a simplistic notion, because every part of the neuronal membrane is capable
of generating action potentials. The extent of the somadendritic back-propagation
of the action potential varies as a function of the excitatory and inhibitory inputs
impinging on the neuron. Because the extracellular spike is a summation of the
integrated signals from both soma and large proximal dendrites, the extracellularly
recorded spike parameters depend on the extent of spike back-propagation
and on other state- and behavior-dependent changes of the membrane potential.
These changes can affect the estimation of the neuron’s virtual “point source” location
and may place the same neuron at different locations, resulting in errors of
spike assignment to neurons.29 Another problem with the point-source assumption
for action potentials is that the somatic origin is not always resolvable with
distant recording sites. For example, in the neocortex, extracellular spikes can be
recorded from the large apical shaft of layer 5 pyramidal neurons as far as 500
micrometers from the cell body. As a consequence, a single electrode tip, placed,
for example, in layer 4, can record equally well from layer 4 cell bodies or apical
dendrites of deeper neurons. These sources of unit sorting errors can be reduced
by recording at multiple sites, using silicon probes, placed parallel to the axodendritic
axis of the neurons.30
104 RHYTHMS OF THE BRAIN
by semiautomatic clustering methods of high-dimensional data. A further difficulty is that no independent
criteria are available for the assessment of omission and commission errors of unit isolation. As
a result, improvement of spike-sorting algorithms is not guided by objective measures. In the absence
of quantitative criteria for unit isolation quality, interlaboratory comparison is difficult and is often a
source for the controversy in data interpretation. This area of research can strongly benefit from future
technical improvements (Henze et al., 2000a; Barthó et al., 2004; Buzsáki, 2004).
31. The combination of the various technical approaches proved successful for the classification of
some interneuron classes in the hippocampus (Csicsvari et al., 1999; Klausberger et al., 2003, 2004).
Separation of Neuronal Classes
Because brain networks consist of several neuronal classes, each with a specific
computation task, their separation on the basis of extracellular features is highly
desirable. Several features of the extracellular spikes may assist with this process,
including spike duration, firing rate and pattern, spike waveform, and the relationship
to network patterns. In the cortex, the most important step is the separation
of pyramidal cells and inhibitory interneurons. This step, in itself, is difficult, and
progress depends on the successful combination of extracellular and intracellular
or other anatomical labeling methods that allow verification of the recorded neuron
types. Separation of the different principal and inhibitory neuronal subclasses
is a further challenge but a necessary requirement for the understanding and interpretation
of assembly cooperation.31
Analyses of Brain Signals
Because neuronal signals have two fundamental appearances, the continuous
membrane potential and field potentials (or analog signals) and the discrete (or
digital) action potentials, their analyses require a combination of methods applicable
for both continuous and point processes. Irrespective of the nature of the
observed signal, brain activity has multiple frequencies and evolves over time.
Therefore, the most appropriate method for analyzing brain signals would be a
“time–frequency analysis” algorithm that would provide a perfect description of
changes in all frequencies as a function of time. However, frequency and time
cannot be mixed; mathematically speaking, they are orthogonal. The implication
is that there is no concept of time in the frequency domain, and conversely, there
is no concept of frequency in the time domain. This counterintuitive relationship
explains why the two major classes of analytical tools that are used for the analyses
of brain signals are called “frequency domain” and “time domain” methods.
Suppose that we would like to “analyze” speech without knowing what information
we are exactly looking for. One approach can characterize the distribution
of frequencies in spoken speech. Because consonants and vowels are composed of
characteristic constellations of frequencies, and because the probability distribution
of the individual consonants and vowels in language is quite different, some
frequencies will dominate the speech frequency landscape. To make sure that all
Windows on the Brain 105
32. Jean Baptiste Joseph Fourier was as impatient as any self-respecting young scientist. In a letter
that he sent together with a paper on algebra to C. Bonard, he complained: “Yesterday was my 21st
birthday; at that age Newton and Pascal had already acquired many claims to immortality.” Fourier’s
immortal fame is mainly due to the method he introduced, the Fourier transform, which expresses a
waveform as a weighted sum of sines and cosines. In essence, it decomposes or separates a waveform
or function into sinusoids of different frequencies that sum to the original waveform. It identifies or distinguishes
the different frequency sinusoids and their respective amplitudes. The Fourier transform of
the autocorrelation function is the power spectrum and is illustrated by a plot of P( f) as a function of f.
33. The temporal resolution of the serial Fourier transform display assumes that the selected segments
are stationary. E.g., when low frequencies are also of interest (e.g., 1 hertz), then the temporal
window of analysis must be slow enough (> 2 × 1 second). For a short overview of spectral methods of
EEG signals, see Muthuswamy and Thakor (1998).
sounds are sampled representatively, long epochs are needed for the analysis.
Alternatively, we can pick a characteristic frequency distribution pattern, determined
in a short time epoch, and examine its distribution over time. Obviously,
neither method alone can reveal the information-encoding scheme of speech. A
frequency domain graph shows only how much of the signal lies within each given
frequency band over a range of frequencies, whereas a time domain display shows
only how a piece of the signal changes over time. EEG, MEG, and even spike-train
signals have both time- and frequency-domain representations that are not fundamentally
different from the example of speech. Understanding the coding ability
of time-evolving neuronal signals, therefore, also requires appropriate methods.
Because brain signals contain multiple frequency components, their relationship
can be quantified using frequency domain methods. The complex EEG or
MEG waveform can be reproduced by an appropriate combination of sine waves.
This method is similar to the trick used by electronic synthesizers that can make
convincing acoustical forgeries of everything from trombones to harps. It is done
by a mathematical process called Fourier synthesis, named after the French mathematician
Joseph Fourier.32 The reverse process, called Fourier analysis, takes the
complex EEG or MEG signal and decomposes it into the sine waves that make it
up. After the signal is decomposed into sine waves, a compressed representation
of the relative dominance of the various frequencies can be constructed. This frequency
versus incidence illustration is known as the power spectrum. The Fourier
method transforms the signal, defined in the time domain, into one defined in the
frequency domain. Although this representation ignores the temporal variation of
the EEG signal, it provides a quantitative answer regarding the power relationship
between the frequencies.
Determining the frequency from any continuous pattern requires making measurements
of time intervals before doing any calculation. However, in complex
waveforms such as the EEG, where multiple frequencies are simultaneously present,
it is often unclear where the intervals to be measured begin and end, that is,
where the analyzed epochs should be. The theory of Fourier transform assumes
that the signal is analyzed over all time—an infinite duration. This restriction suggests
that the standard Fourier analysis is not well suited to describing timetransient
changes in “frequency content” because the frequency components
defined by the Fourier transform require infinite time support.33
106 RHYTHMS OF THE BRAIN
A practical “solution” to the time versus frequency orthogonality issue is the
short-time Fourier transform, which attempts to quantify the frequency content
changes over time. In this modified analysis, the brain signal is divided into multiple
short epochs, and the Fourier transform is calculated for each epoch. The
successive spectra can display the evolution of frequency content with time.
The short-time Fourier transform can be viewed as a compromise of joint
time–frequency analysis. The use of a wide temporal window will give good frequency
resolution but poor time resolution, whereas the use of a narrow window
will give good time resolution but poor frequency resolution. Accepting this compromise,
these modified methods can analyze sequential short epochs, and the
frequency structure can be displayed as a function of time (figure 4.8).
Another popular way of analyzing short-time segments of selected EEG patterns
is called “wavelet” analysis. The wave refers to the fact that this function is
Figure 4.8. Frequency spectral dynamics of an EEG. Top: Gray-scale–coded power of
neocortical local field potential in the rat during REM (rapid eye movement) sleep, slow
wave sleep, and waking exploration. Arrows indicate volume-conducted theta frequency
(7–9 hertz) oscillation from the underlying hippocampus. Bottom: Fourier spectra of
lumped epochs. The asterisk indicates the power peak in the sleep spindle band (12–18
hertz). Note the distinct theta and gamma (40–90 hertz) frequency power increase during
REM sleep and waking. The spectra have been “whitened” by removing the 1/f correlated
power (see Cycle 5 for explanation of this term). LFP, local field potential. Figure courtesy
of Anton Sirota.
Windows on the Brain 107
34. Wavelet transforms are broadly classified into the discrete wavelet transform and the continuous
wavelet transform. The discrete transform uses a specific subset of all scale and translation values,
whereas the continuous transform operates over every possible scale and translation.
35. “The wavelet transform replaces one single, poorly behaved time series with a collection of
much better behaved sequences amenable to standard statistical tools” (Percival and Warden, 2000).
Although both wavelet and Hilbert transformation methods have become widely used recently, these
methods are formally (i.e., mathematically) identical with the Fourier transform (Bruns, 2004).
oscillatory; the diminutive form refers to the fact that this (window) function is of
finite length or a fast-decaying, oscillating waveform. The wavelet transform
refers to the representation of a signal in terms of a finite length. Rather than analyzing
the distribution of all frequencies, the wavelet transform first selects a “frequency
of interest.”34 Any arbitrary waveform can serve as a “prototype” whose
consistent appearance can be quantified. Therefore, all wavelet transforms may be
considered to be forms of time–frequency representation.35
Analysis in the time domain is usually performed by temporal correlation
functions. Correlating a signal with itself is called autocorrelation, which can reveal
repetitive components in the signal. Because different sorts of signals have
distinctly different autocorrelation functions, these functions can be used to tell
signals apart. For example, random noise is defined as uncorrelated because it is
similar only to itself, and any small amount of temporal shift results in no correlation
with the unshifted signal at all. In contrast, oscillating signals go in and out
of phase when shifted in time. The autocorrelation function of a periodic signal is
itself a periodic signal, with a period identical to the original signal. Therefore,
autocorrelation is an effective method for revealing periodic function in a signal.
Correlation methods are also often used to assess the similarity between two signals.
When two signals are similar in shape and are in phase (i.e., “unshifted”
with respect to each other), their correlation is positive and maximum. The same
signals out of phase will provide a negative maximum correlation. As one signal
is shifted with respect to the other and the signals go out of phase, the correlation
decreases. Correlating a signal with another, that is, computing their crosscorrelation,
is a powerful method for detecting a known reference signal in noise
or the directional connectedness of neuron pairs (figure 4.7).
Forms of Oscillatory Synchrony
There are numerous ways to affect oscillators, and the mechanisms that modify
them do not even have a standardized taxonomy. Some of the terms have agreed
mechanisms within one discipline but are used differently in another. Here are
definitions of a few terms as they are used in neuroscience and in the neurocomputation
literature. Mutual entrainment refers to a measure of stability of two or
more oscillators that they would not have on their own. Mutual feedback is the
key to entrainment of oscillators of various frequencies and stabilities. For example,
when multiple single-cell oscillators with different intrinsic frequencies are
108 RHYTHMS OF THE BRAIN
Figure 4.9. Collective synchronization of multiple oscillators. The state of each oscillator
is represented as a dot in the complex plane. The amplitude and phase of the oscillation
correspond to the radius and angle of the dot in polar coordinates. Gray scale codes the oscillators’
natural frequencies. In the absence of coupling, each oscillator would settle onto
its limit cycle (circle) and rotate at its natural frequency. Starting from a random initial
condition, the oscillators self-organize by adjusting their amplitudes and are pulled toward
the mean field (asterisk) that they generate collectively; then, they sort their phases so that
the fastest oscillators are in the lead. Ultimately, they all rotate as a synchronized pack,
with locked amplitudes and phases. Reprinted, with permission, from Strogatz (2001).
36. Coherence is a measure of phase covariance, which is quantified as the cross-spectrum of two
signals divided by the product of the two autospectra. Because it measures spectral covariance, it cannot
reliably separate amplitude and phase contributions. Phase-locking statistics can quantify phase
coherence between two signals independent of the amplitudes of the respective signals (Lachaux et al.,
1999).
37. For statistical methods of phase-locking, see Hurtado et al. (2004).
connected together, they may produce a common intermediate global frequency.
This is not a simply linear sum of the frequencies because in the functionally
connected scheme each neuron fires phase-locked to the global rhythm. If one
generator temporally exceeds the common tempo, the others absorb the excess,
forcing it to slow down. Conversely, if it falls below the population rhythm, the
other oscillators “pull” it back so that it catches up. The emergent population
rhythm enslaves or supervenes the behavior of individual units (figure 4.9).
Coherence is the measure of the state in which two signals maintain a fixed
phase relationship with each other or with a third signal that serves as a reference
for each.36 The phase differences are often used to infer the direction of the force,
although in most cases such inference is not possible.
Phase-locking refers to the mutual interaction among oscillators by which the
phase difference between any two oscillators is fixed. This measure is independent
of the amplitude changes.37 Phase-locking or phase-coupling can occur between
oscillatory and nonoscillatory events as well, such as phase-locked
Windows on the Brain 109
38. In addition, physics describes a variety of dynamical synchronization phenomena that may
have relevance to neuronal networks, including splay-phase states, collective chaotic behavior, attractor
crowding, clustering, frequency locking, dephasing, and bursting (Tass, 1999).
discharge of irregularly spiking neuron and an oscillator. Often, the term entrainment
is used for such cases.
Cross-frequency phase synchrony can occur between two or more oscillators
of different integer frequencies, when the oscillators are phase-locked at multiple
cycles. If two oscillators differ in frequency and cannot fix their phases, they nevertheless
can produce a transient and systematic interaction, called phase precession
or phase retardation.
Phase reset of one or many coupled or independent oscillators can occur if a
punctuate input forces to restart the oscillator(s) at the same phase. A related phenomenon
but one involving different mechanisms, is phase synchronization, a
stimulus-induced oscillation in two or more structures with transient coherent phase.
Phase modulation of power occurs between a slower and faster rhythm, where the
power of the faster oscillator varies as a function of phase of the slower oscillator.
Induced rhythms at two or multiple locations can be brought about with or
without phase synchrony by a slowly changing input. Even if the oscillators are
not coherent with each other, their power can be comodulated. This is sometimes
referred to as amplitude envelope correlation, since the integrated envelopes of
the signals are compared. It conveniently allows comparison of the amplitudes of
any frequency band. Large-frequency mismatch or strong coupling can lead to oscillation
death or quenching. This short list of terms is far from exhaustive but
sufficient for the understanding of most of the oscillatory phenomena discussed in
this volume.38
Naive wisdom would demand that one should use all methods described in this
Cycle, in all possible combinations and in every experiment to get the best result.
However, no serious mentor would provide such foolish advice to a student. The
reason is that each of these techniques is very complex, and a thorough understanding
of each requires several years of hard work and study. Furthermore, simultaneous
application of multiple methods is often detrimental for various technical
reasons. So when it comes to the important question of choosing the best method
for understanding the brain, I pass along the advice I learned from my professor of
pathology György Romhányi: “The best method for investigating any issue is your
method.” Choose one or two or three methods and learn all their pitfalls and virtues
so you can interpret your data better than anyone else. There is no “best” method.
Briefly . . .
Because the brain is organized at multiple temporal and spatial levels, monitoring
brain activity requires methods with appropriate resolutions. To date, only a handful
of recording methods are available, and none of them has the ability to “see”
110 RHYTHMS OF THE BRAIN
simultaneously small and large areas at the temporal resolution of neuronal activity.

Field potential analysis (EEG and MEG), imaging of energy production in
brain structures (fMRI), optical recording methods, and single-cell recording
techniques are the principal techniques in contemporary cognitive-behavioral
neuroscience for the study of the intact brain. Unfortunately, even their combined,
simultaneous application in behaving subjects falls short of the goal of
explaining how a coalition of neuronal groups generates representations of the
environments and creates appropriate responses in a changing environment. In
the brain, specific behaviors emerge from the interaction of its constituents, neurons
and neuronal pools. Studying these self-organized processes requires the simultaneously
monitoring of the activity of large numbers of individual neurons
in multiple brain areas. Development of large-scale recording from multiple single
neurons with tetrodes or silicon probes is an attempt in this direction. However,
these methods are invasive and cannot be used for the investigation of the
healthy human brain. Many other methods, such as pharmacological manipulations,
macroscopic and microscopic imaging, and molecular biological tools, can
provide insights into the operations of the brain, but in the end all these indirect
observations should be reconverted into the format of neuronal spike trains to
understand the brain’s control of behavior.
Cycle 5
A System of Rhythms: From Simple to
Complex Dynamics
Ts’ui Pen... did not think of time as absolute and uniform. He believed in an
infinite series of times, in a dizzily growing, ever spreading network of diverging,
converging and parallel times. This web of time—the strands of
which approach one another, bifurcate, intersect, or ignore each other . . .—
embraces every possibility.
—Jorge Luis Borges, “The Garden of Forking Paths”
111
Neurons and connections of the brain support and limit its self-generated, spontaneous
order even in the absence of sensory inputs or motor outputs. As described
in Cycles 2 and 3, its structural organization supports a high complexity of wiring
architecture. However, not all neurons and connections are used all the time.
Quite the contrary, only a small fraction of the rich possibilities are chosen at any
one moment. The dynamically changing functional or effective connectivity gives
rise to short-lived oscillations that are perpetually created and destroyed by the
brain’s internal dynamics. The central tenet of this Cycle, which is echoed
throughout the book, is that brain dynamics constantly shift from the complex to
the predictable.1 Neuronal ensemble activities shuttle back and forth between the
interference-prone complexity and robust predictable oscillatory synchrony. As I
explain in this Cycle, this switching behavior is the most efficient way for the
brain to detect changes in the body and the surrounding physical world, while
preserving its autonomous internal organization.
1. Karl Friston emphasized the importance of short-lived transients in his “labile brain” series (Friston,
2000). According to Friston, brain dynamics move from a stable incoherence through dynamic instability
to complete entrainment. A similar idea is echoed by the chaotic organization of Walter Freeman’s
“wave packets” (Freeman and Rogers, 2002; Freeman et al., 2003) and the “neural moment” of transient
synchrony of Hopfield and Brody (2001). It is not clear, though, how stable incoherence (high entropy)
can be maintained in an interconnected system, e.g., the brain. As Sporns et al. (2000a, b, 2002) have
pointed out, high-complexity and high-entropy conditions require very different architectures.
112 RHYTHMS OF THE BRAIN
2. The first comprehensive review on the subject is Katz and Cracco (1971).
3. See International Federation of Societies for Electroencephalography and Clinical Neurophysiology
(1974) and Steriade et al. (1990a).
4. Komisaruk (1970) has already suggested that the different brain and body oscillators are coupled
by some mechanisms, but he assumed an integer phase-locked relationship between them.
How Many Brain Oscillators?
Since the seminal discoveries of Hans Berger (Cycle 1), oscillations have been
documented in the brains of numerous mammalian species, ranging from very
slow oscillations with periods of minutes to very fast oscillations with frequencies
reaching 600 hertz.2 Somewhat surprisingly, a functionally meaningful taxonomy
of brain rhythms has not emerged until recently. The first classification, introduced
by the experts of the International Federation of Societies for Electroencephalography
and Clinical Neurophysiology in 1974, was driven by pragmatic
clinical considerations.3 Following Berger’s tradition, the subsequently discovered
frequency bands were labeled with Greek letters, and the borders between
the different bands were evenly and arbitrarily drawn (delta, 0.5–4 hertz; theta,
4–8 hertz; alpha, 8–12 hertz; beta, 12–30 hertz; gamma, > 30 hertz), like the
straight-line country borders between the African nations drawn by the colonialists.
The frequency border classification was done out of necessity, since the
mechanisms and independence of the various oscillatory patterns were largely
unknown at that time. The frequency coverage of the classified bands was con-
fined by the EEG recording technology. The widely used mechanical pen
recorders limited the upper border of frequencies, whereas electrode polarization
and movement artifacts prevented routine observations at low frequencies. Thus,
frequency bands below 0.5 hertz were not included or given names. Although the
international classification of brain frequency bands continues to be of practical
importance, its major disadvantage is its self-limitation. Rhythms generated by
the same physiological machinery at different ages or in different species often
fall into different bands with different names. For example, the hippocampal theta
oscillation was discovered in the anesthetized rabbit, and because of its frequency
coverage (2–6 hertz), the name “theta” was given. However, in the drug-free rodent,
hippocampal theta should be designated theta-alpha, according to the committee’s
recommendation, since it varies between 5 and 10 hertz.
A useful taxonomy of brain oscillations would require that the individual oscillatory
classes represent physiological entities that are generated by distinct mechanisms.
The same mechanism giving rise to different frequency bands in different
species or the same frequency bands in different states (e.g., sleep/awake, anesthesia)
of the same species ought to be referred to by the same name, even though the
dynamics underlying the rhythms may be different. Unfortunately, the exact mechanisms
of most brain oscillations are not known. As an alternative approach, Markku
Penttonen, a postdoctoral fellow in my lab, and I speculated that there might
be some definable relationship among the various brain oscillators.4 Penttonen
A System of Rhythms 113
5. Our strategy of taxonomic classification of brain rhythms followed the tactic used by Dmitri
Mendeleev for his construction of the periodic chart of elements. In 1860, Mendeleev attended the
First International Chemical Congress in Karlsruhe, Germany, where the leading chemists of the day
gathered to sort out contradictory lists of atomic and molecular weights. They left for home with un-
finished business. However, the 34-year-old Mendeleev left the meeting with a grand research project
in his mind: find a meaningful system among the known elements. Playing a jigsaw puzzle with the elements
for nine years did not yield results. Yet, one night in sleep, “I saw in a dream a table where all
the elements fell into place as required,” remembered Mendeleev (Strathern, 2000). His taxonomy
provided organization to inorganic chemistry.
6. Buzsáki et al. (1992), Penttonen and Buzsáki (2003), and Buzsáki and Draguhn (2004).
7. The described system of oscillators is characteristic of the cerebral cortex only. Most other
brain areas can support only limited types of oscillations.
reasoned that, if we found a quantifiable relationship among the well-documented
few, perhaps we could make some predictions about the less-known ones.5
We began by looking at the relationship among three hippocampal rhythms
observed in the rat: theta (4–10 hertz) and gamma (30–80 hertz) rhythms and a
fast oscillation (140–200 hertz).6 These rhythms are independently generated, because
we have already observed that gamma oscillations persist without theta and
compete with the fast oscillation. Beginning with these three rhythms, we tried to
interpolate and extrapolate other classes and relate them to each other. We found
the best fit based on a natural logarithmic scale. Using the mean frequencies of
our initially identified rhythms, the mean frequencies of other oscillation classes
were estimated. The predicted frequencies corresponded to the traditional beta
and delta bands as well as to less-known slow oscillations that we designated slow
1, slow 2, slow 3, and slow 4. By plotting the frequency bands in increasing order
of frequency, a general principle emerged: discrete oscillation bands formed a
geometric progression on a linear frequency scale and a linear progression on a
natural logarithmic scale (figure 5.1, bottom).7 This simple graph allowed us to
make some fundamental statements about brain oscillators. First, all frequencies
from 0.02 hertz to 600 hertz are continuously present, covering more than four
orders of magnitude of temporal scale. Second, at least 10 postulated distinct
mechanisms are required to cover the large frequency range. Third, because a single
structure does not normally generate all oscillatory classes, structures must
cooperate to cover all frequencies. Different mechanisms in different brain structures
can give rise to the same oscillatory band, but there should be at least one
distinct mechanism for each oscillation class. Fourth, and perhaps most important,
there is a definable relationship among all brain oscillators: a geometrical
progression of mean frequencies from band to band with a roughly constant ratio
of e, 2.17—the base for the natural (Napierian) logarithm. Since e is an irrational
number, the phase of coupled oscillators of the various bands will vary on each
cycle forever, resulting in a nonrepeating, quasi-periodic or weakly chaotic pattern:
this is the main characteristic of the EEG.
All of this raises important questions: why are there so many oscillators? Why
can the brain not use a single, fixed-frequency clock for all of its functions? There
are multiple answers to these questions. Behavior occurs in time, and precise tim-
114 RHYTHMS OF THE BRAIN
ing from fractions of a second to several seconds is necessary for successful prediction
of changes in the physical environment and for the coordination of muscles
and sensory detectors in anticipation of environmental events. In principle, multiple
tasks can be managed by a precise, single, fast clock and time division, as is
seen in digital computers. Perhaps a de novo design of the mammalian brain would
choose this solution. However, for sponges and other simple animals at early
stages of evolution, fast responding was not a requisite for survival. All that is
needed in these simple creatures is slow rhythmic movements for providing food
frequency (Hz)
Classes
Power
0.05
4 3 2 1 0
0
0.5
0
0.5
1.5
1
1 2
200-600 Hz, ultra fast
80-200 Hz, fast
3 4 5 6 lnHz
1 2 3
In frequency (Hz)
4 5 6
0.37 2.72 20.09 148.41
30-80 Hz, gamma
10-30 Hz, beta
4-10 Hz, theta
1.5-4 Hz, delta
0.7-2 s, slow 1
2-5 s, slow 2
5-15 s, slow 3
15-40 s, slow 4
Figure 5.1. Multiple oscillators form a hierarchical system in the cerebral cortex. Top:
Power spectrum of hippocampal EEG in the mouse recorded during sleep and waking periods.
Note that the four peaks, corresponding to the traditional delta, theta, gamma, and fast
(“ripple”) bands, are multiples of natural log integer values. Bottom: Oscillatory classes in
the cerebral cortex show a linear progression of the frequency classes on the log scale. In
each class, the frequency ranges (“bandwidth”) overlap with those of the neighboring
classes so that frequency coverage is more than four orders of magnitude. The power spectrum
was “whitened” by removing the log slope dominating the typical EEG spectrum (e.g.,
figure 5.2). Modified, with permission, from Penttonen and Buzsáki (2003).
A System of Rhythms 115
8. A nice example is category learning (McClelland et al., 1995).
9. It was perhaps William James (1890) who first pointed to the segmentation of experience:
The unit of composition of our perception of time is a duration, with a bow and a stern, as it
were—a rearward- and a forward-looking end. It is only as parts of this duration-block that the
relation of succession of one end to the other is perceived. We do not first feel one end and then
feel the other after it, and from the perception of the succession infer an interval of time between,
but we seem to feel the interval of time as a whole, with its two ends embedded in it.
(p. 609)
James’s observer is at an instant but embedded in the stretched time of the mind.
intake. Once a slow oscillator was invented, faster ones could be added as needed
in subsequent evolutionary stages. New inventions of evolution are always built on
the back of previously useful functions. Another argument for not using a single
fast clock has to do with the wiring of the brain and the way neurons communicate
with each other. Although action potentials, the digital means for communication
between neurons, propagate in nerves innervating muscles relatively
quickly (tens of meters per second), most axon collaterals in the brain are fairly
slowly conducting (from centimeters up to a few meters per second). This sluggishness
likely reflects an economical compromise of evolution between size and
speed. Thicker axons conduct faster but occupy more space. But saving space
comes with a price. For instance, informing multiple postsynaptic targets, located
between 0.5 millimeter and 5 millimeters, of a single neuron may take 1 and 10
milliseconds, respectively; an order of magnitude time difference between the
most proximal and most distant target! This problem becomes progressively
larger when more complex events are represented in increasingly large neuronal
ensembles. Oftentimes, the cooperative activities of hundreds of neurons are
needed to discharge their postsynaptic target. The arrival times of action potentials
from such a large number of sources must be coordinated in time to exert an
impact. Recognizing somebody’s face and recalling her first and last names, her
profession, our last meeting, and our common friends are events that do not occur
simultaneously but are protracted in time, since larger and larger neuronal loops
must become engaged in the process. A number of psychological phenomena argue
in favor of the idea that these cognitive events require hierarchical processing.8
Separate processing requires the engagement of neuronal networks at
multiple spatial scales.
Each oscillatory cycle is a temporal processing window, signaling the beginning
and termination of the encoded or transferred messages, analogous to the beginning
and end signals of the genetic code. In other words, the brain does not
operate continuously but discontiguously, using temporal packages or quanta.9
Designers of general-purpose programmable computers recognized a long time
ago that networks with cycles have orders of larger capabilities than networks
without cycles (e.g., a feed-forward net; see Cycle 9). The wave length of the oscillatory
category determines the temporal windows of processing (figure 5.1)
and, indirectly, the size of the neuronal pool involved. It follows from this speculation
that different frequencies favor different types of connections and different
levels of computation. In general, slow oscillators can involve many neurons in
116 RHYTHMS OF THE BRAIN
10. The filtering property of the brain tissue is the standard explanation for the observation that the
noise at a given frequency f is spatially correlated over a distance L( f) that increases as f decreases
(Voss and Clark, 1976). The physicist Paul Nuñez pioneered rigorous applications of physical wave
propagation theories to brain waves (Nuñez, 1998). Physics provides a vast toolbox for treating wave
phenomena mathematically. These techniques have provided some understanding of global brain phenomena
in terms of the physical properties of its carrier medium. How far can we go with this physicist’s
view of the brain? While medium filtering is an important factor, it cannot explain the larger
spatial extent of neuronal recruitment at lower frequencies or the behavior-dependent highly coherent
gamma oscillations in distant brain areas (König et al., 1995; Varela et al., 2001; Buzsáki et al., 2004).
11. Benjamin Libet’s brain stimulation experiments support this point. Libet’s principal finding
was that short trains of pulses evoked only unconscious functions, and the somatosensory cortex had
to be stimulated for 200–500 milliseconds for evoking a conscious sensation of touch. To become
aware of a sensory experience requires engagement of the appropriate brain networks for hundreds of
milliseconds. The delay between Libet’s “mind time” relative to physical time is a favorite argument
of philosophers to question the unity of the mind and brain (Libet, 2004).
12. Von Stein et al. (1999) and Sarnthein et al. (1998).
large brain areas, whereas the short time windows of fast oscillators facilitate local
integration, largely because of the limitations of the axon conduction delays.10
Computation in the brain always means that information is moved from one
place to another. Obviously, the path length of network connectivity is very critical
in this process. Because the synaptic path length (recall the degree of neuron
separation, as defined in Cycle 2) and effective connectivity determine the possible
routes for shuttling information from structure to structure, the cycle lengths
(i.e., periods) of the oscillation limit how far information gets transferred in one
step. Fast oscillations, therefore, favor local decisions, whereas the involvement
of distant neuronal groups in distinct structures in obtaining a global consensus
requires more time.11 This principle is nicely illustrated by a series of experiments
by Astrid von Stein and Johannes Sarnthein at the University of Zurich. In
their first experiment, they had human subjects view parallel grating stimuli with
different numbers of bars per degree of visual field. Their main finding was that
the power of the lower gamma-frequency band (24–32 hertz) increased with the
number of bars per degree. Importantly, these changes were confined to the primary
visual cortex. In the second experiment, everyday objects, familiar to all
sensory modalities, were shown instead. Each object was presented as spoken
word, written word, and picture. The modality-independent processing of inputs
resulted in increased coherent activity between the adjacent temporal and parietal
cortices. The main synchronization took place in the beta frequency range (13–18
hertz). A third set of experiments tested verbal and visuospatial working memory.
This time synchrony was observed between the prefrontal and posterior associational
cortices in the theta range (4–7 hertz). Although the extent of active neuronal
assemblies could not be determined by this approach, the findings
nevertheless support the idea that the size of the activated neuronal pool is inversely
related to the frequency of synchronization.12 The forthcoming Cycles
discuss and attempt to justify these ideas in detail. For now, let us tentatively accept
that the several oscillatory classes have distinct mechanisms, each serves a
A System of Rhythms 117
13. From Latin circa (about) and di (day), meaning “about a day.” Diurnal and nocturnal refer to
patterns during the day and night, respectively, whereas ultradian rhythms are shorter periodic
changes, also locked to the 24-hour cycle. The discipline of chronobiology is fully devoted to the
study of body time, the impact of cyclic variations on health and disease (“chronotherapy”).
14. Liu and Reppert (2000) have shown that synchronization of suprachiasmatic neurons is mediated
mostly by GABA acting on GABAA receptors.
different function, and each involves various magnitudes of neuronal pools. Because
many of these oscillators are active simultaneously, we can conclude that
the brain operates at multiple time scales.
Ultradian and Circadian Rhythms
The oscillators discussed so far are brain and neuron specific and emerge primarily
through mechanisms that are unique to neurons. However, several other
rhythms affect brain activity at a much slower pace, the most famous of which is
the circadian rhythm with a 24-hour period.13 As is the case with most oscillators,
circadian periodicity can be maintained without an outside influence. The
hypothalamic suprachiasmatic nucleus is usually referred to as the circadian
“pacemaker” in mammals because it controls the daily fluctuations in body
temperature, hormone secretion, heart rate, blood pressure, cell division, cell regeneration,
and the sleep/wake periods. Unlike members of most network oscillators,
each of the 20,000 neurons in the human suprachiasmatic nucleus is a
circadian oscillator. This alone is not a unique feature of these neurons, since the
molecular mechanisms that sustain the 24-hour rhythm are present in every cell
of the body, although each cell in isolation would run a bit faster or slower than
the 24-hour circadian cycle. The free-running periods of the individually isolated
suprachiasmatic neurons in tissue culture vary from 20 to 28 hours, with
firing patterns of some cells or groups with 6- to 12-hour phase shifts. In the intact
brain, individual cells are entrained into coherent oscillation likely through
their connectivity. As is the case in many other neuronal oscillators, the inhibitory
neurotransmitter GABA and gap junction communication among the
inhibitory neurons appear essential for the synchronization of individual
neurons.14
What make the circadian clock so “slow” are the molecular mechanisms involved
in its generation. It takes about 4–6 hours to make a protein from the gene.
Internal timing is achieved through a complex feedback loop in which at least four
freshly produced proteins participate. Two proteins, active in the morning, begin
to produce a second set of molecules that accumulate during the day. In the evening,
this second set of proteins inactivates the daylight-active proteins. The inactivation
process involves genes in the nucleus. For example, in the fruit fly
(Drosophila), a messenger RNA is transcribed from the period (per) gene, which
in turn initiates the production of PER protein. The accumulating protein in the
118 RHYTHMS OF THE BRAIN
15. The positive elements of this loop in the mouse are the transcription factors CLOCK and
BMAL1, which drive three period genes (mPer1–mPer3) and one cryptochrome gene (mCry1). The
mPER and mCRY proteins form heterodimers and negatively regulate their own transcription (Kume
et al., 1999). Other key reports in this field include Konopka and Benzer (1971), Menaker et al.
(1978), Pickard and Turek (1983), and Allada et al. (2001). The book by Foster and Kreitzman (2004)
is an entertaining introduction to the daily cycles of life.
16. The quote is from a book (Los Autonautas de la Cosmopista) by the Argentine writer Julio
Cortazar written together with his wife, Carol Dunlop, on a 33-day trip from Paris to Marseilles, as
cited in Golombek and Yannielli (1996).
17. A useful collection on subcircadian rhythms in the body is in the volume edited by Hildebrandt
(1957).
cytoplasm enters the nucleus and suppresses further messenger RNA production.
The result is a reduction of PER with reduced suppression of the messenger RNA
production, and the cycle can start again. The real picture is a lot more complex
and involves interactions among the proteins themselves and multiple autoregulatory
transcriptional/protein translational feedback loops.15
On the input side, timing of the circadian clock can be delayed or advanced by
light, which in mammals is detected by the retina in the eye. In the retina, a small
group of scattered ganglion cells contain the photoreceptor melanopsin and ambient
light directly makes these neurons fire. The exclusive brain target of this special
group of light-detecting neurons is the suprachiasmatic nucleus. Phase-locking
of this “master” circadian clock leads to the production of hitherto unidentified
molecules that work as output signals and synchronize the cycling of all individual
cells in the body. In contrast to “simple” relaxation oscillators, several daily
pulses may be required to bring the multiple partners of the circadian clock—
including the sleep/wake cycle, body temperature, hormone secretion, and physical
and mental functions—to a full reset and phase synchrony. All travelers are
aware of this synchronization problem. “When you go [from America] to Europe,
the soul takes about three days longer to get there,” noted the writer-traveler Julio
Cortazar.16
Embedded within the circadian cycle are at least two well-documented ultradian
rhythms. The faster one recurs at approximately 90–100 minutes, whereas
the mean duration of the slower one is 3–8 hours, with the shorter component
superimposed upon the longer one.17 Studies of the circadian and subcircadian
rhythms form a new and fast-growing discipline that is gaining increased attention
in medicine, psychiatry, and sleep research. What interests us most in the
context of this Cycle is the relationship between these molecular oscillators and
the faster neuronal rhythms. The observation that isolated single neurons of the
suprachiasmatic nucleus vary their firing rates, emitting about twice as many
spikes during the light phase as in the dark phase, proves the existence of a
mechanism that translates molecular changes to spiking output. These output
spikes can affect other neuronal oscillators in other parts of the brain. On the
feedback side, suprachiasmatic neurons are affected not only by light but also
by the global activity of the brain. For example, following sleep deprivation
A System of Rhythms 119
18. Deboer et al. (2003) found that changes in vigilance states are paralleled by strong changes in
the spiking activity of suprachiasmatic neurons and concluded that the circadian clock can be substantially
modified by afferent information from the brain.
there is a rebound of prolonged sleep. In turn, such rebound sleep activity has a
profound effect on the activity of suprachiasmatic neurons.18 It is also signifi-
cant that extension of the natural logarithmic relationship among neuronal
oscillators extrapolates faithfully to the periods of ultradian and circadian
rhythms.
In the past, brain oscillators were studied in isolation. Recently, we have begun
to see them as part of a system of oscillators with an intricate relationship between
the various rhythmic components. Considering the short path lengths of
anatomical connectedness in the cerebral cortex, this complexity may not be surprising.
Nevertheless, future systematic work is needed to decipher the general
rules and mechanisms of coupling among the neuronal rhythms occupying multiple
spatial and temporal scales.
The 1/f Statistical Behavior of EEG
One grand question about the brain is how the microscopic laws of cell discharges
and synaptic activity can lead to a complex system organized at multiple time
scales. The inverse relationship between oscillation classes and the magnitude of
neuronal recruitment provides some interesting clues about the brain’s long-time
and large-scale behavior. When a goal is scored in a football stadium, the coordinated
roar of fans can be heard for miles, in contrast to uncoordinated local conversations,
which are lost in the background noise. Similarly, slow rhythms
involve very large numbers of cells and can be “heard” over a long distance,
whereas localized fast oscillations involving only a small fraction of neurons may
be conveyed only to a few partners. The “loudness” feature of the various network
oscillations can be quantified easily by Fourier analysis (Cycle 4). Once the signal
is decomposed into sine waves, one can construct a power spectrum of the frequencies,
a compressed representation of the relative dominance of the various
frequencies. Although power spectrum ignores the temporal variation of the signal,
it provides a quantitative assessment of the power relationship between the
frequencies. In figure 5.2, the logarithm of the density is plotted against the logarithm
of the EEG frequency. In this so-called log–log plot, we see a straight line,
the hallmark of scale-free systems (i.e., systems that obey power laws; Cycle 2).
By and large, the amplitude (square root of power), A, increases as the frequency,
f, decreases, as expressed by the inverse relationship, A ∼ 1/fα, where α is an exponent.
The physicist’s reaction to such a relationship is that the EEG reflects
nothing special except the internal “noise” of the brain, generated by its active
and passive components. At first glance, this conclusion seems diametrically
120 RHYTHMS OF THE BRAIN
opposite to our suggestion above that the brain generates a large family of oscillations
that allows for processing and predicting events at multiple time scales. Random
noise does not allow any prediction. However, the noise with the “one over
f ” power spectrum is a special noise (also called “pink” noise).
A critical aspect of brain oscillators is that the mean frequencies of the neighboring
oscillatory families are not integers of each other. Thus, adjacent bands
cannot simply lock-step because a prerequisite for stable temporal locking is
phase synchronization. Instead, the 2.17 ratio between adjacent oscillators can
give rise only to transient or metastable dynamics, a state of perpetual fluctuation
between unstable and transient phase synchrony, as long as the individual oscillators
can maintain their independence and do not succumb to the duty cycle influence
of a strong oscillator.19 In the parlance of nonlinear dynamics, the oscillators
are not locked together by a fixed point or attractor (phase), but they attract and
repel each other according to a chaotic program and never settle to a stable attractor.
A main reason for this recklessness is the presence of multiple oscillators that
perpetually engage and disengage each other. Locally emerging stable oscillators
in the cerebral cortex are constantly being pushed and pulled by the global dynamics.
Nevertheless, despite the chaotic dynamics of the transient coupling of
log frequency (Hz)
log power
0 0.5 1 1.5 2
Figure 5.2. Power spectrum of EEG from the right temporal lobe region in a sleeping human
subject (subdural recording). Note the near-linear decrease of log power with increasing
log frequency from 0.5 to 100 hertz, the characteristic feature of “pink” or “complex”
noise. The arrow indicates the peak at alpha (∼11 hertz). Reprinted, with permission, from
Freeman et al. (2000).
19. For a didactic explanation of chaotic coupling of oscillator pairs and their ability to generate
metastable saddle dynamics, see Bressler and Kelso (2001).
A System of Rhythms 121
20. The exponent of the 1/fα relationship varies somewhat across frequencies (Leopold et al.,
2003; Stam and de Bruin, 2004) and behavioral conditions (eyes open vs. eyes closed) but is highly invariant
across subjects (Linkenkaer-Hansen et al., 2001).
21. 1/f noise is ubiquitous in nature. In white noise, the power density is constant over a finite frequency
range [P(f) = constant power]. If we mix visible light with different frequencies at random, the
resulting light is white, hence the name “white noise.” It is also known as Johnson noise. If the different
frequencies are mixed according to 1/f distribution, the resulting light is pinkish. In pink noise, the
power density decreases 3 decibels per octave with increasing frequency (density proportional to 1/f)
over a finite frequency range that does not include direct current. Engineers use the terms “flicker” or
the oscillators at multiple spatial scales, a unified system with multiple time scales
emerges. Indeed, the inverse relationship between frequency and its power is an
indication that there is a temporal relationship between frequencies: perturbations
of slow frequencies cause a cascade of energy dissipation at all frequency scales.
One may speculate that these interference dynamics are the essence of the global
temporal organization of the cortex.
In most illustrations, the log–log linear relationship breaks off below 2 hertz
(figure 5.2). Does this mean that frequencies below 2 hertz follow a different rule?
This departure from the 1/f line is partially due to the high-pass filtering feature
of the routinely used amplifiers. However, if slow frequencies are also part of the
scale freedom, they should have an impact on higher frequencies. Indeed, longterm
scalp recordings confirm power-law scaling behavior for all frequencies
tested and expand the temporal scale of the 1/f line beyond a minute.20 This relationship
indicates that amplitude fluctuation of, for example, an alpha wave at this
instant in your occipital cortex can influence the amplitude of another alpha wave
a thousand cycles later and all waves in between.
The scale-invariant feature of the EEG is the mathematical telltale sign of selforganization.
The speed at which the power decreases from low to high frequencies
measures the length of the correlations or, using another phrase, the
“temporal memory effects” in the signal. This time memory effect is the main
reason why the 1/f relationship is so intriguing. If there were no relationship
among the frequency bands, the power density would be constant over a finite frequency
range and the spectrum would be flat, 1/f0
. Physicists call this pattern
“white” noise. So there must be other colors of noise.
The third type of noise is “brown” noise. This time the term refers to the biologist
Robert Brown, the discoverer of the cell nucleus, who also observed pollen
particles performing a random dance in a water droplet: Brownian motion. In the
case of brown noise, the power density decreases much faster with frequency
(1/f2
) than is the case for pink noise. Brown noise is random at longer intervals,
but it is easily predictable and strongly correlated at short intervals. For example,
while touring a city without a guide or plan, we make turns at random at the intersections
but our walk in straight streets is predictable (“random walk” pattern).
Now, the interesting conclusion we can draw from this crash course on noise is
that the 1/f behavior of EEG and magnetoencephalographogram (MEG) is the
golden means between the disorder with high information content (white noise)
and the predictability with low information content (brown noise).21 The cerebral
122 RHYTHMS OF THE BRAIN
“excess noise.” In brown noise, the power density decreases 6 decibels per octave with increasing frequency
(density proportional to 1/f 2
) over a frequency range that does not include direct current. In
brown noise, each point is displaced by a Gaussian (distributed) random amount from the previous
point. This is also known as “random walk” noise (Mandelbrot, 1983). For an accessible introduction
to the 1/fα and its more general form, the 1/xα behavior of various living and physical systems, see
Gardnera (1978). For psychologists and behavioral scientists, I recommend the review by Gilden
(2001).
22. Such a rare form of traveling wave is the so-called K-complex of light sleep (Massimini et al.,
2004). For spiral waves and vortices, see Ermentrout and Kleinfeld (2001).
23. Studies using multiple extracellular and intracellular studies over a large portion of the cat
neocortex by Steriade and colleagues provide ample evidence for the synchronous nature of slow
(delta and slow 1) oscillations (Steriade et al., 1993b, d, e; see Steriade, 2001a, 2003).
24. For a detailed treatment of the physics of EEG, see Nuñez (1998).
cortex with its most complex architecture generates the most complex noise
known to physics. But why would the brain generate complex noise?
The brain-specific problem to be explained is why the power increases towards
lower frequencies. The physicist-engineer explanation is that brain tissue acts as a
capacitive filter so that the faster waves are attenuated more than are slow waves.
This cannot be the whole story, however, because another main feature of the
spectrum, namely, that perturbations of slow frequencies result in energy dissipation
at all frequency scales, cannot be easily explained by discrete oscillators and
passive filtering. Brain oscillators are not independent, however. In fact, the same
elements, neurons, and neuronal pools are responsible for all rhythms. However,
when the rhythm is fast, only small groups can follow the beat perfectly because
of the limitations of axon conductance and synaptic delays. Slower oscillations,
spanning numerous axon conduction delay periods, on the other hand, allow the
recruitment of very large numbers of neurons. Thus, the slower the oscillation,
the more neurons can participate; hence, the integrated mean field is larger. With
local connections only, an emerging rhythm at one place would progressively invade
neighboring territories, resulting in traveling waves.22 At other times, the
rhythm would emerge simultaneously at several locations and might be synchronized
via the intermediate and long-range connections.23 In short, the inevitable
delays and the time-limited recruitment of neuronal pools can account for a good
part of the 1/f magic.24
All of these EEG frequency relations would, of course, be of minimal interest
even to oscillator aficionados if they were not intimately connected to behavior. If
noise generation is simply a byproduct of brain operations, an inevitable inconvenience
that has to be overcome, then we might sit back and simply marvel at the
brain’s extraordinary ability to compete with its self-generated noise. Alternatively,
correlated noise production could be a deliberate “design” that must have
important advantages and perceptual, behavioral consequences. From the latter
viewpoint, the brain not only gives rise to large-scale, long-term patterns, but
these self-organized collective patterns also govern the behavior of its constituent
A System of Rhythms 123
25. The effects of the emergent, higher level properties on the lower level ones are often called
“downward causation” or an emergent process (Thompson and Varela, 2001). Haken (1984) and Kelso
(1995) refer to these features of dynamic systems by the term “circular causality.” There is no identifi-
able agent responsible for the emergent organization; nevertheless, the pattern dynamics of the system
can be mathematically described by the “order parameter,” emergent property or “relational holism.”
26. Voss and Clark (1975).
27. Interestingly, the complex predictability of sounds applies to monkeys, dogs, and other animals,
as well, in which the pleasantness–annoyance dimension can be behaviorally measured. The
psychologist Anthony Wright claims that rhesus monkeys hear music and other sounds in much the
same way as humans do. His monkeys reliably identified the melodies of two songs, “Happy Birthday
to You” and “Yankee Doodle Dandy,” even when they were separated by as many as two octaves
(Wright et al., 2000). For a review on “music perception” in animals, see Houser and McDermott
(2003). Of course, the dynamics of cortical patterns in all mammals exhibit 1/f spectra.
neurons.25 In other words, the firing patterns of single cells depend not only on
their instantaneous external inputs but also on the history of their firing patterns
and the state of the network into which they are embedded. Complex systems
with 1/f behavior can be perturbed in predictable ways by various inputs. This
susceptibility should apply to the brain, as well. Thus, it should not come as a surprise
that power (loudness) fluctuations of brain-generated and perceived sounds,
like music and speech, and numerous other time-related behaviors exhibit 1/f
power spectra. Perhaps what makes music fundamentally different from (white)
noise for the observer is that music has temporal patterns that are tuned to the
brain’s ability to detect them because it is another brain that generates these patterns.
The long-time and large-scale note structure of Bach’s First Brandenburg
Concerto is quite similar to the latest hit played by a rock station or to Scott
Joplin’s Piano Rags.26 On the other hand, both high temporal predictability, such
as the sound of dripping water, and total lack of predictability, such as John
Cage’s stochastic “music” (essentially white noise) are quite annoying to most
of us.27
If self-generated brain dynamics have a link to the spectral composition of
speech and music, one might expect that the same dynamics would influence a
plethora of other behaviors. Indeed, in addition to speech and music, the power
law function is the best fit to the large data sets available on forgetting in humans
and on other time-related behavioral patterns in a range of species, including habituation,
rate sensitivity, and the many properties of time-based reinforcement
effects and even synchronization errors of human coordination. Let us focus on
some of them in more detail.
Weber’s Psychophysical Law and Large-Scale
Brain Dynamics
The anatomical-functional organization of the cerebral cortex should have
consequences and limitations on cognitive behaviors as well. A well-known
124 RHYTHMS OF THE BRAIN
28. Ernst Heinrich Weber noted that the increase of stimulus necessary to produce an increase of
sensation in the various modalities is not a fixed quantity but depends on the proportion that the increase
bears to the immediately preceding stimulus. The generalization of the relationship between
physical stimuli and cognitive events has come to be known as psychophysics. Gustav Theodor Fechner,
working at the same university (Leipzig) but unaware of Weber’s work, described the same law
but stated it in an equivalent mathematical form. When he learned that Weber had already discovered
the relationship, he generously referred to his own observations as a consequence of Weber’s law. Often,
psychologists honor both by calling the relationship the Weber-Fechner law. Long before the
Weber-Fechner law, the Pythagoreans recognized that human perceptions of differences in musical
pitch correspond to ratios of vibrating string or air column lengths. It is believed that Pythagoras himself
discovered that pleasing chords are achieved if length ratios correspond to successive elements of
an arithmetic progression, 1/2, 2/3, and 3/4, which define, respectively, the octave, fifth, and fourth
(see Curtis, 1978; Schwartz et al., 2003). Contemporary psychophysical research has refined and replaced
Weber’s law by a more precise Stevens’s power law. Sensations (S) are related to the physical
stimulus (P) as S =Pn
, where n could be less than 1 but occasionally greater than 1 (Stevens, 1975).
29. In its broad “definition,” quale is the qualitative content of experience, e.g., pleasure, pain, sorrow,
and the feelings that emanate from perception of color, sound, etc. (Llinás, 2001; Tononi, 2004).
According to the philosopher’s definition, it is an attribute of something that we observe in our minds
(e.g., Searle, 1992; Dennett, 1987).
30. Excellent books and papers deal with behavioral assessment of time in animals (Church and
Gibbon, 1982; Killeen and Weiss, 1987; Staddon and Higa, 1999) and humans (Vierordt, 1868; Allan,
1979; Gibbon et al., 1984; Levin and Zakay, 1989; Tallal, 2004; Näätänen and Syssoeva, 2004).
psychophysical law that comes to mind in connection with the 1/f nature of cortical
EEG is that of Weber and Fechner: the magnitude of a subjective sensation (a
cognitive unit) increases proportionally to the logarithm of the stimulus intensity
(a physical unit). For example, if a just-noticeable change in a visual sensation is
produced by the addition of one candle to an original illumination of 100 candles,
10 candles will be required to detect a change in sensation when the original illumination
is 1,000 candles.28 According to Rodolfo Llinás at New York University,
Weber’s law also underlies the octave tonal structure of music perception and production.
He goes even further by suggesting that quale,29 the feeling character of
sensation, may “derive from electrical architectures embedded in neuronal circuits
capable of such logarithmic order.” If so, then the 1/f dynamics may be the
functional architecture underlying qualia and without the ability of a proper architecture
to generate such temporal dynamics, no “feelings” can be generated
(see Cycle 13 for a more extended discussion of this topic).
In the behavioral literature, interval or duration timing is often explained by a
discrete pacemaker-accumulator mechanism that, similar to a stop watch, yields a
linear scale for encoded time.30 However, researchers have been aware of problems
related to the intuitively simple ticking clock. The fundamental problem is
that timing with a single clock implies similar accuracy at all time intervals; that
is, the coefficient of variation (standard deviation divided by the mean) should not
increase. Behavioral observations, however, show that the error of the hypothesized
internal clock is proportional to the clock time; that is, they follow Weber’s
law or Stevens’s power law, much like large-scale brain dynamics as measured by
the EEG signal. Because the exponent of the power rule for interval magnitude
and interval production errors is close to 1 (i.e., it is pink noise), some authors
A System of Rhythms 125
31. However, several investigators argue in favor of a characteristic “tempo” in both music (beat)
and speech. Syllables are generated every 200–400 milliseconds during continuous speech in all languages.
32. Nieder and Miller (2003), examining the variability of single-unit activity from the monkey
prefrontal cortex, concluded that encoding of numerical information follows Weber’s law. They
trained monkeys to discriminate between different numbers of dots. Control behavioral experiments
showed that the monkeys indeed counted from 1 to 5. When plotted on a logarithmic scale, the tuning
curves of “number neurons” could be fitted by a Gaussian with a fixed variance across the range of
numbers tested. For “number neurons” in the parietal cortex, see Sawamura et al. (2002). The increasing
magnitude of neuronal pool necessary for identifying higher numerosity can explain the scaling.
33. The earliest large scalp-recorded response is a negative potential (N1, 60–80 milliseconds after
stimulus onset) followed by positive deflection (P1) at about 100 milliseconds. These “sensory” potentials
are localized to the primary sensory cortical areas of the appropriate modality. The N2–P2
complex (around 200 milliseconds) is also known as mismatch negativity (Näätänen et al., 1987) because
its amplitude is sensitive to the frequency mismatch of regular signals. Localization of the components
is more difficult with scalp recordings, and they are collectively referred to as “cognitive”
components. The most studied “cognitive” potential, so-called P300 (Sutton et al., 1965), is enhanced
after an unexpected “oddball” event is embedded in a series of familiar events. A later, N450 (450 milliseconds)
component is believed to reflect semantic encoding (Kutas and Hillyard, 1980). It is important
to note that these evoked components reflect averaged waveforms of hundreds of repetitions. The
single events can often be equally or better described as combinations of various oscillators. See Uusitalo
et al. (1996) and Cycle 10.
argue that psychological time corresponds to real time, at least at the milliseconds
to seconds scale. The psychophysical observations also indicate that there is not a
certain point in this time continuum where timing is most accurate. In other
words, time perception does not have a characteristic time scale; it is scale-free.31
This may be because the brain, in contrast to computers and other single clockdependent
machines, uses a complex system of multiple oscillators for its operations
with a power (1/f) relationship among them.
The progressively longer time required for recalling items from short-term
memory after the initial fast recall of the first items may also reflect properties of
systems with pink noise. An intuitive explanation of the storage-limiting effect in
the brain is the time–space propagation of activity. Longer times allow propagation
of activity to an ever-increasing population of neurons. However, information
passing through spatially divergent neuronal networks is progressively more vulnerable
to interference from other network effects (technically referred to as noise
or “leakage”), therefore information deteriorates over time.32 Evoked-potential
experiments, recorded from the human scalp, nicely illustrate this conjecture.
Sensory stimuli, such as flashes of light, evoke progressively longer latency,
longer duration, lower amplitude, and more variable responses at successive
stages of sensory pathways. Repeated presentation of such stimuli leads to modi-
fication (e.g., habituation) of the evoked responses. The most vulnerable components
are the long-latency responses recorded from higher level associational
areas, whereas the short-latency components, reflecting activity of early processing,
are quite resistant to habituation.33 The observations in humans echo
earlier experiments in cats. When the source and intensity of the auditory conditioning
signal were changed, the latency and amplitude of the early evoked
126 RHYTHMS OF THE BRAIN
34. Grastyán et al. (1978).
35. Several recent experiments provide evidence for the scale-free and spatial fractal nature of
scalp EEG recorded over extended durations (Freeman and Rogers, 2002; Freeman et al., 2003; Gong
et al., 2003; Hwa and Ferree, 2002; Leopold et al., 2003; Le van Quyen, 2003; Linkenkaer-Hansen et
al., 2001; Watters, 1998; Stam and de Bruin, 2004).
36. Even the power spectrum of synaptically isolated neurons, generating intrinsic (channel)
noise, has a 1/f form (DeFelice, 1981; White et al., 2000).
37. The scale invariance of fractals implies that the knowledge of the properties of a model system
at short time or length scales can be used to predict the behavior of the real system at large time and
length scales. In our context, the EEG pattern recorded from a single site for a “representative” period
of time can predict the EEG periods for very long times and at any other recording sites. Although neither
of these statements holds true across all time and spatial scales, understanding the rules of space
and time invariance of EEG is important.
responses faithfully reflected the parameters of the physical stimulus. However,
the amplitude and shape of longer latency responses were essentially independent
of the location and intensity of the signal source and were, instead, invariant concomitants
of the significance of the signal, as verified by the overt behavior of the
cats.34 Overall, these observations suggest that successive stages of information
processing have distinct and characteristic memory decays.35
The Fractal Nature of EEG
So far, we have tacitly assumed that the distribution of EEG and MEG power at
different frequencies obeys the same rule, irrespective of the recording position in
the brain or whether activity was monitored in a relatively small or very large
neuronal pool. Indeed, this assumption appears to be the case, at least to a certain
minimum spatial scale. Power spectra of long epochs of electrical fields, representing
membrane voltage fluctuations of perhaps a few hundred neurons in the
depth of the cortex when recorded by a microelectrode (micrometer range) or
millions of neurons recorded by scalp electrodes (∼ 10 centimeters), are essentially
identical. Furthermore, the spectral content and frequency bands of the human
EEG and the electrocorticogram of mice, rats, guinea pigs, rabbits, cats,
dogs, and monkeys are remarkably similar. In other words, the long-term temporal
structure of the macroscopic neuronal signal, reflecting the collective behavior
of neurons that give rise to it, is macroscopically by and large similar in virtually
all cortical structures and in brains of various mammalian species. This is a remarkable
observation. In essence, the claim is that a collective pattern recorded
from a small portion of the cortex looks like the pattern recorded from the
whole.36 This “scale invariance” or “self-similarity” is a decisive characteristic of
fractals.37 Fractal structures—such as river beds, snow flakes, fern leaves, tree arbors,
and arteries—and fractal dynamic processes—such as pink noise, cloud formation,
earthquakes, snow and sand avalanches, heart rhythms, and stock market
price fluctuations—are self-similar in that any piece of the fractal design contains
a miniature of the entire design. Regarding the collective behavior of neuronal
A System of Rhythms 127
38. Claims about the fractal nature of the brain are in fact quite old. The importance of localized
vs. global or holistic brain operations is a long-standing controversy in philosophy, psychology, and
neuroscience (Lashley, 1931). The presence of 1/f behavior indicates that the network dynamics are
both local and global.
39. Kelso’s book on the dynamical system properties of the brain (Kelso, 1995) is an important
step in this direction.
signals as fractals with self-similar fluctuations on multiple time and geometry
scales has potentially profound theoretical and practical implications for understanding
brain physiology. It implies that the macroscopic EEG and MEG patterns
describe the large-scale function of neuronal networks as a unified whole,38
independent of the details of the dynamic processes governing the subunits that
make up the whole.
The concept that physical systems, made up of a large number of interacting
subunits, obey universal laws that are independent of the microscopic details is a
relatively recent breakthrough in statistical physics. Neuroscience is in serious
need of a similar systematic approach that can derive mesoscale laws at the level
of neuronal systems.39 The scale freedom of spatial and temporal dynamics in the
cortex has emerged as a useful direction of research. Does this mean that some
universal laws using a tiny bit of mathematics can help to bring the neuronal algorithms
out into the light?
Pausing with this thought for a second, the math is not as simple as it looks.
The seductively simple 1/fα function is, in fact, a very complex one. Every new
computation forward takes into consideration the entire past history of the system.
The response of a neuron depends on the immediate discharge history of the
neuron and the long-term history of the connectivity of the network into which it
is embedded. Assuming 100 independent neurons with spiking and nonspiking
binary states, more than 1030 different spike combinations are possible. However,
only a very small fraction of these combinations can be realized in the brain because
neurons are interconnected; thus, they are not independent constituents. As
a result, even a weak transient local perturbation can invade large parts of the network
and have a long-lasting effect, whereas myriads of other inputs remain ignored.
Although neuronal networks of the brain are in perpetual flux, due to their
time-dependent state changes, the firing patterns of neurons are constrained by
the past history of the network. Complex networks have memory.
Scale-Free Dynamics of Noise and Rhythms:
From Complexity to Prediction
The novel spectral analysis methods and the mathematics of fractals and power
laws have not only helped reveal the large-scale behavior of the brain signals but
have also led to some fierce debate about the relationship between brain oscillations
and noise. At the heart of the debate is the question of whether brain dynamics
are characterized best by the various oscillators or “simply” pink noise.
128 RHYTHMS OF THE BRAIN
40. This issue has been debated for quite some time, and there are prominent people on both sides
of the debate (e.g., Wright and Liley, 1996; Nuñez, 2000; Shadlen and Newsome, 1994, 1995). According
to Erb and Aertsen (1992), “the question might not be how much the brain functions by virtue
of oscillations, as most researchers working on cortical oscillations seem to assume, but rather how it
manages in spite of them. (p. 202).”
41. The now classic mathematical thesis by Bak et al. (1987) combined two fashionable
concepts—self-organization and critical behavior—to explain the even more difficult notion of complexity.
This short paper’s seductive claim is that, if a system has 1/f temporal scale and spatially fractal
features, its behavior does not require any external “tuning” to undergo phase transitions (in
contrast to e.g., water-ice phase transition that does require the external influence of temperature). Instead,
complex systems spontaneously evolve to a state, where they lose their characteristic temporal
and spatial scales, the result of which is that correlations run through the system at all scales. Selforganized
criticality provides a definition of complexity: a system that exhibits 1/f and spatially fractal
statistics. Complex systems with self-organized criticality include snow avalanches, earthquakes,
forest fires, size of cities, airport traffic, Internet communication, blackouts in electric networks, size
of companies, and biological mass extinctions. The attractive feature of the self-organized criticality
hypothesis is that the statistical properties of these complex systems can be described by simple power
laws. Several recent studies have suggested that EEG dynamics are characterized by self-organized
criticality. See, e.g., Linkenkaer-Hansen et al. (2001), Freeman et al. (2003), Le van Quyen (2003),
and Stam and de Bruin (2004).
The prominent alpha rhythms in human scalp recording notwithstanding, power
spectra of long EEG segments, recorded over various behaviors, give rise to spectra
without clear peaks. Rhythms come and go at various frequencies and various
times, and their effect may average out in the long term. The feeling of permanence
is only an illusion, and brain rhythms are no exception. Does this all mean
that the recorded brain rhythms are simply extreme states of the neuronal noise
generated by the busy brain works?40 If EEG synchronization between different
brain regions does not have a characteristic time scale, it is hard to understand
how the effective connectivity of those regions can be modified according to rapidly
changing behavioral needs.
The scale-free nature of global synchronization dynamics implies some specific
predictions. One such explicit implication of the 1/f law is that, most times,
brain dynamics are in a state of “self-organized criticality.” This mathematically
defined complex state is at the border between predictable periodic behavior and
unpredictable chaos. In the context of brain dynamics, the implication of the concept
of self-organized criticality is that the cerebral cortex displays perpetual state
transitions, dynamics that favor reacting to inputs quickly and flexibly. This
metastability is a clear advantage for the cerebral cortex since it can respond and
reorganize its dynamics in response to the smallest and weakest perturbations.
However, noise can be defined only in a finite temporal window, and the 1/f dynamics
of brain activity are deduced from long temporal integration windows.
Yet, at every instant, the state of the network is different; thus, the ability of cortical
networks to respond to perturbation also changes from moment to moment.
A direct prediction of the self-organized criticality theory is that rare but extremely
large events are inevitable, because at one point 1/f dynamics become supersensitive
to either external perturbations or its internal processes, responding
with very large synchronized events.41 One may rightly mistrust this latter claim.
A System of Rhythms 129
42. Jensen (1998) is an excellent and entertaining short summary of the highlights and downsides
of the self-organized criticality theory.
In the lifetime of a normal brain, such unusually large events never occur, even
though the ability of neuronal networks to generate such avalanches is illustrated
by the supersynchronous activity of epileptic patients. The tensegrity dynamics of
excitation and inhibition guard against such unexpected events. We have to recall
that the EEG reflects the “average” behavior of neurons, with many interacting
degrees of freedom. In the complex system of the brain, many degrees of freedom
interact at many levels, such as neurons, mini- and macromodules, areas, and systems.
Seemingly insignificant changes of the interactive constituents at any level
can dramatically affect the course of events, as real-world tests of the self-organized
criticality theory illustrate. For example, experiments with sand piles, rice piles,
and other systems indicate that some minor changes of boundary conditions and
space constants can often switch their critical dynamics to oscillatory behavior.
For example, rice piles of certain types of rice grains display a broad distribution
of avalanche sizes, thus supporting the theory. Sand piles, on the other hand, most
often evolve into a temporal periodic state, presumably because gravity (a constant)
can overcome the friction between sand grains.42
Another prediction of the postulated scale-invariant and spatial fractal feature
of EEG is that knowledge of short-time information can be used to calculate longrange
temporal correlations; similarly, knowledge about small-space scale information
can estimate the global state. Neither prediction works perfectly well, as is
demonstrated by numerous experiments in later Cycles. The rhythm versus the
1/f noise controversy should remind us of the somewhat banal, but nevertheless
important, fact that general concepts such as power laws may be able to capture
some aspect of a phenomenon without necessarily being able to explain all of its
critical details. The 1/f feature of the EEG is obvious only when the events are integrated
over a sufficiently long time and at a large enough spatial scale.
Why do some see convincing 1/f behavior, and others see mostly rhythms in
EEG and MEG traces? Can the power spectrum be decomposed into individual
rhythms generated by distinct neurophysiological mechanisms, or should we look
for mechanisms that can generate pink noise without oscillations? Luckily, one
can address the rhythm versus 1/f statistics controversy by removing the noise
from the measured brain signal obtained during a particular behavior. This process
is usually referred to as “whitening” or precoloring of the power spectrum by
removing the correlated “pink” noise. The illustration shown in figure 5.1 (top) is
an already whitened spectrum, which is why we can see clearly separable peaks at
delta, theta, gamma, and fast (“ripple”) frequencies. If there were no discrete, albeit
interdependent, oscillations, the trace would be flat. Recall now that our logarithmic
rendering of brain oscillators (figure 5.1, bottom) shows that in the
frequency range where the 1/f relationship is continuous (2–200 hertz), five distinct
oscillatory bands exist, each of which has a wide range of frequencies that
fluctuate over time. The frequency, amplitude, and recurrence variability of the
oscillators may account for the smoothness of the broad frequency range of the
130 RHYTHMS OF THE BRAIN
43. A somewhat similar idea is expressed by Szentágothai and Érdi (1989).
44. For the relationship between Fourier analysis and wavelet methods, see Cycle 4.
45. Stam and de Bruin (2004) also point out that investigators who do not report characteristic
time scales in the EEG typically analyze long recording epochs with large variability. In contrast,
studies that consistently report on distinct oscillations tend to sample short epochs of EEG signals.
power spectrum constructed from long recordings without a need for generating
extra noise.43 Put bluntly, the brain does not generate complex noise directly. Instead,
it generates a large family of oscillations whose spatial-temporal integration
gives rise to the 1/f statistics. This is, in fact, the simplest way of producing
complex noise. The bonus is that switching from complexity to the predictive oscillatory
mode can be fast; such a transition is a major requirement for efficiently
selecting a response from a background of uncertainty.
In the debate between pink noise and rhythms, we also have to remember how
we generate the power spectrum and what we are trying to answer with it. Recall
that the Fourier analysis works in the frequency domain and ignores temporal
variations altogether. The power spectrum of Bach’s First Brandenburg Concerto
is the same, regardless of whether it is played forward, backward, or chopped into
short segments and mixed so that even the best Bach scholars fail to recognize the
masterpiece. Fast but critical transitions across patterns cannot be recognized in
long-time power spectra. This, of course, applies to brain signals as well. All important
serial effects, reflecting sequences of overt and covert behavior, are ignored
by the summed power spectrum of the EEG and MEG. To compensate for
such serious shortcomings, improved methods, such as the short-time Fourier
transform or wavelet analysis, have been introduced. With these improved methods,
sequential short epochs can be analyzed and the frequency structure displayed
as a function of time.44 This procedure is equivalent to calculating the
power spectrum of the score in the First Brandenburg Concerto at every few hundred
milliseconds. Obviously, there is still a lot of arbitrariness in the procedure,
but it is a significant improvement over integrating the whole concert together
over time. The most important advantage of such refined time series analysis is
that now spectral characterization of EEG or MEG can be done in time windows
that more faithfully correlate with behavioral changes. Using such refined
brain–behavior analysis, spectra that correspond to identical behaviors can be
combined and their frequency–power distributions can be contrasted across different
behaviors. When the analysis is carried out in such a way, the presence of
rhythms and their association with overt and cognitive behaviors often become
obvious.45 The simple reason is that transient behavioral changes and responses
are often associated with characteristic but transient oscillations.
A similar contrast applies to the coherence of EEG activity as a function of
distance. Long-term observations consistently show that coherence of neuronal
activity rapidly decreases as a function of distance at high frequency but deceases
less for low frequencies. On the other hand, short-lived but highly coherent
oscillations in the gamma frequency band are often reported between distant
sites processing different but related aspects of inputs (discussed in Cycle 10).
A System of Rhythms 131
46. Sensory, motor, and cognitive event-related “desynchronization” of the scalp EEG is just such
a clear example of the perturbation of the postulated critical state of the brain. See discussion in Cycles
6 and 7.
47. For further arguments in favor of the importance of state variability in cognition and stimulusinduced
effects, see Arieli et al. (1996), Friston (2000), Gilden (2001), and Buzsáki (2004).
These observations are important, because if the occurrence of a behavioral act
is consistently associated with an induced rhythm in some structures, it likely
bears physiological importance. In the critical state, the spatiotemporal correlations
of neuronal interactions make the brain highly susceptible to perturbations,
allowing for an instantaneous reorganization of effective connectivity.46
Perturbations, such as sensory stimuli or motor output, could reduce the critical
state and provide transient stability by oscillations. These transient stabilities of
brain dynamics are useful to hold information for some time, as is the case while
recognizing a face or dialing a seven-digit telephone number. Shifting the brain
state from complex pink-noise dynamics to a state with a characteristic temporal
scale is therefore an important mechanism that provides a transient autonomy
to various levels of neuronal organization. I suggest that the ability to
rapidly shift from the state of metastable pink noise to a highly predictable oscillatory
state is the most important feature of cortical brain dynamics. In the
high-complexity (1/f) regime of metastability, the brain is in a critical state capable
of responding to weak and unpredictable environmental perturbations. By
shifting its dynamics to an oscillatory regime, it instantaneously creates a state
with linear variables, which is a fundamental physiological requirement for psychological
constructs described by the terms “anticipation,” “expectation,” and
“prediction.”
Because most overt and covert behaviors are transient, their brain oscillation
correlates are also expected to be short-lived. Thus, it would appear that averaging
short-time power spectra is the perfect way to analyze brain–behavior relations.
Indeed, stimulus-evoked averaging of brain potentials or metabolic changes
in brain imaging experiments has been a standard procedure in cognitive and experimental
psychology. The variability of the responses across trials is generally
downplayed as unexplained variance or “noise” that needs to be averaged out to
reveal the brain’s true representation of invariant input. In functional magnetic
resonance imaging (fMRI), the responses are often pooled and averaged across
subjects to further reduce the variance. The real problem with the averaging procedure,
of course, is that the state of the brain is constantly changing. State
changes are hard to predict from behavior on a moment-to-moment basis. State
variability is, to a large extent, internally coordinated. This “correlated brain
noise,” as it is often referred to, might be critically important because it is a potential
source of mental operations.47 The recorded signal, in fact, may contain
more information about the observer’s brain state than about the input, because
the process is an “interpretation” or “construction” rather than a reflection, to use
terms borrowed from psychology. In order to predict the present state of a brain,
one needs to have access to its recent history